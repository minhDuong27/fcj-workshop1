[
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Workshop Report: \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; Workshop Objectives Gain an overview of the AI/ML landscape and how AWS is adopted in Vietnam Understand the full ML workflow using Amazon SageMaker Explore Generative AI capabilities through Amazon Bedrock Practice prompt engineering and RAG (Retrieval-Augmented Generation) Learn how to build practical AI/ML solutions using AWS services Event Information Location: AWS Vietnam Office Time: 8:30 AM – 12:00 PM, Saturday, November 15, 2025 Speakers \u0026amp; Organizing Team Instructors:\nLâm Tuấn Kiệt – Senior DevOps Engineer, FPT Software Presented the SageMaker overview and general AWS ML services Đinh Lê Hoàng Anh – Cloud Engineer Trainee, FCAJ, Swinburne University of Technology Covered Amazon Bedrock and AWS AI/ML service ecosystem Danh Hoàng Hiếu Nghị – Fresher AI Engineer, Renova Cloud Demonstrated Bedrock Agent Core and guided the hands-on practice Coordinators:\nAWS Vietnam Community Team FCJ (First Cloud Journey) Program Leads Agenda Overview 8:30 – 9:00 AM: Registration \u0026amp; Kickoff Participant check-in and early networking Introduction to the workshop structure and expected outcomes Ice-breaker session A quick look at the AI/ML market and adoption trends in Vietnam 9:00 – 10:30 AM: AWS AI/ML Overview – Deep Dive into Amazon SageMaker Amazon SageMaker – The End-to-End ML Platform\nData Preparation \u0026amp; Labeling:\nData Wrangler for cleaning and transforming datasets Ground Truth for labeling and annotation Feature Store for centralizing and reusing features Training, Tuning \u0026amp; Deployment:\nUse built-in algorithms or bring your own training scripts Run hyperparameter tuning jobs Deploy models via real-time, batch, or serverless inference Experiment with A/B testing models and multi-model endpoints Built-in MLOps Capabilities:\nSageMaker Pipelines to automate ML workflows Model Registry for tracking versions and governance Model Monitor to detect drift and quality issues CI/CD integration for continuous deployment Live Demo – SageMaker Studio:\nLaunch a notebook instance Train a sample ML model Deploy an endpoint and send test requests 10:30 – 10:45 AM: Coffee Break Short break for refreshments Casual Q\u0026amp;A with AWS engineers 10:45 AM – 12:00 PM: Generative AI with Amazon Bedrock \u0026amp; AWS AI/ML Services AWS AI/ML Services Overview\nRekognition – Image/video analysis Translate – Neural machine translation Textract – Document text extraction Transcribe – Speech-to-text Polly – Natural-sounding text-to-speech Comprehend – NLP and text analytics Kendra – Intelligent search engine Lookout – Industrial anomaly detection Personalize – ML-powered recommendations Foundation Models: Claude, Llama, Titan\nModel Selection Insights: Claude: Strong in reasoning-heavy conversation tasks Llama: Good for customization, open-source flexibility Titan: Amazon-native, cost-effective, integrated with AWS How to choose depending on use case Prompt Engineering Essentials\nCore Prompting Techniques:\nProvide clear instructions and meaningful context Use few-shot examples Apply chain-of-thought for complex logic Use role-based prompting to guide model behavior Advanced Prompting:\nTuning temperature and token limits Knowing when to use system vs user prompts Template-based prompting for reusability Retrieval-Augmented Generation (RAG)\nRAG Architecture:\nEmbeddings + vector search Semantic retrieval before generating answers Feeding retrieved context back into prompts Knowledge Base Integration:\nBedrock Knowledge Bases Store documents in Amazon S3 Connect to data sources (S3, DBs, external APIs) Best practices for chunking and metadata Set correct bucket policies for secure access Amazon Bedrock Agent Core\nBuilding Autonomous Agents:\nMulti-step task planning and orchestration Action groups for calling APIs Persistent memory for context retention Tool Integration with Lambda:\nLambda as an execution engine for custom logic Real-time data processing Querying databases or external systems Serverless approach → simplified scaling and maintenance Guardrails for Safe AI\nContent filtering and moderation PII detection and redaction Topic restrictions and safety rules Custom guardrails for enterprise policies Live Demo – Creating a GenAI Chatbot with Bedrock\nEnabling foundation model access Designing prompts for a basic chatbot Adding RAG with Knowledge Bases Applying guardrails to control responses Testing and fine-tuning the bot Key Insights From Amazon SageMaker A complete platform that covers the entire ML lifecycle Well-integrated MLOps tools Easy scaling from experimentation to production Flexible pricing models to control cost From Amazon Bedrock Variety of foundation models readily available Prompt engineering plays a major role in result quality RAG helps integrate enterprise knowledge effectively Guardrails are essential for safe deployment Agent Core enables multi-step intelligent workflows Practical Lessons Start with the business problem, not the tool Prototype quickly using SageMaker Studio Use foundation models before investing in custom training Guardrails ensure safety and compliance Always monitor performance and optimize workloads Applying the Knowledge Experiment in SageMaker Studio using small datasets Build a RAG-based chatbot using Bedrock + S3 Practice prompt engineering with different models Automate ML pipelines with SageMaker Pipelines Try building simple Bedrock Agents for internal workflows Implement guardrails before deploying any AI app Share learnings with teammates to improve team-wide standards Personal Experience Attending the “AI/ML/GenAI on AWS Workshop” at the AWS Vietnam Office was a hands-on and insightful experience. The mix of explanations, demos, and real-world examples made the content easier to understand.\nLearning from AWS Experts Clear explanations on SageMaker’s end-to-end workflow Solid demos of Bedrock and real GenAI applications Many Vietnam-based use cases provided good context Practical advice on choosing the right AWS tools Hands-on Demonstrations Observed the full ML process: data → training → deployment Learned how Bedrock simplifies building GenAI applications Applied prompt engineering tricks in real examples Understood how RAG makes LLMs more accurate with enterprise data Saw how Agents organize multi-step tasks Understanding AI/ML Trends Differences between traditional ML and GenAI became clearer Better idea of when to pick SageMaker vs Bedrock Understood the importance of proper MLOps for production systems Networking Met many developers and data enthusiasts exploring AWS AI/ML Exchanged experiences about real-world implementation challenges Built new connections with AWS community members Important Takeaways Foundation models significantly reduce development effort Good prompts dramatically improve model output RAG is essential for knowledge-heavy chatbot applications Guardrails are not optional—they are required for safe AI SageMaker is ideal for long-term, scalable ML projects Next Steps Continue exploring SageMaker Studio hands-on Build a small RAG proof-of-concept using Bedrock Practice prompt engineering with multiple foundation models Experiment with Bedrock Agents for automation workflows Learn more about MLOps and monitoring practices Engage with the AWS AI/ML community for ongoing learning Event Pictures Overall, this workshop provided a comprehensive introduction to AWS AI/ML services, from traditional machine learning with SageMaker to cutting-edge Generative AI with Bedrock. The hands-on demonstrations and expert guidance made complex concepts accessible and immediately applicable. The key takeaway is that AWS provides a complete ecosystem for building, deploying, and scaling AI/ML applications, making it easier than ever to bring AI innovations to production.\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": "Enabling customers to deliver production-ready AI agents at scale by Swami Sivasubramanian on 16 JUL 2025 in Amazon Bedrock, Amazon Connect, Amazon Nova, Amazon Q, Amazon Simple Storage Service (S3), Announcements, AWS Inferentia, AWS Trainium, AWS Transform, Featured, Thought Leadership\nAI agents are poised to have an impact as transformative as the internet itself, enabling automation, solving complex problems, and driving innovation. However, bringing AI agents into production at scale requires more than just training large models—it demands an architecture that ensures security, reliability, scalability, and flexibility.\nAWS addresses this challenge with a microservices-based architecture: each agent capability (memory, identity, observability, tool access, customization) is encapsulated as a small, independent service, loosely coupled via a hub. This decomposition improves agility, makes it easier to add or replace components, and helps organizations focus on delivering business value instead of building infrastructure from scratch.\nArchitecture Guidance Compared to monolithic approaches, AWS decomposes agent functionality into multiple microservices:\nAgentCore: the serverless runtime that orchestrates agents.\nFunctional services: memory, identity, observability, gateway, code interpreter.\nModel customization service: Amazon Nova fine-tuning microservice.\nThis design allows customers to freely choose models, integrate existing data sources, and connect seamlessly with open-source frameworks.\nTechnology Choices and Communication Scope Communication scope Technologies Within a single microservice AWS Lambda, Step Functions Between microservices in one agent Amazon SNS, AWS CloudFormation cross-stack references Between services/agent Amazon EventBridge, API Gateway, AWS Cloud Map The Pub/Sub Hub The pub/sub hub pattern is central to AWS’s approach:\nEach microservice interacts only with the hub, not directly with other microservices.\nResults, errors, or intermediate outputs are pushed back into the hub for further processing.\nBenefits: reduces synchronous calls, scales easily, keeps components loosely coupled.\nDrawback: requires monitoring and coordination to prevent misrouted or duplicate messages.\nCore Microservice The foundation of the solution, providing the data and communication layer:\nAmazon S3 for data and artifacts.\nAmazon DynamoDB for metadata and catalog.\nAWS Lambda for consistent writes and runtime logic.\nAmazon SNS Topic as the central hub.\nThis ensures all writes to the data lake and catalog are controlled and consistent.\nFront Door Microservice The primary entry point for external requests:\nAmazon API Gateway provides a REST interface.\nAmazon Cognito (OIDC) manages authentication and authorization.\nDeduplication mechanism built with DynamoDB, avoiding SNS FIFO limitations and enabling proactive duplicate detection.\nProcessing Microservices Agents often require specialized tools such as browsing, code execution, or search. These are implemented as microservices:\nLambda triggers subscribe to the hub and filter messages.\nStep Functions orchestrate pipelines (e.g., preprocessing, model invocation).\nLambda functions execute specific parsing or transformation logic.\nResults or errors are returned to the hub for downstream services.\nModel Customization Microservice (Nova) Fine-tuning is handled as a dedicated microservice:\nSupports both full fine-tuning and parameter-efficient techniques.\nMethods include SFT, DPO, RLHF, CPT, and Knowledge Distillation.\nRuns independently, so updates or retraining do not disrupt the overall runtime.\nNew Features in the Solution 1. Amazon Bedrock AgentCore Serverless runtime for agents.\nSession isolation and observability.\nIntegrates with open-source frameworks.\n2. Expanded Tooling Support Microservices for memory, identity, gateway, browser, and interpreter.\nEach is modular and replaceable.\n3. Customer Adoption Enterprises like Itaú Unibanco, Innovaccer, Boomi, Box, and Epsilon are already adopting this architecture for scalable production deployment. Outputs: AgentCoreTopic: Value: !Ref AgentCoreTopic Export: Name: !Sub ${AWS::StackName}-AgentCoreTopic AgentMemoryTable: Value: !Ref AgentMemoryTable Export: Name: !Sub ${AWS::StackName}-AgentMemoryTable NovaCustomizationJob: Value: !Ref NovaCustomizationJob Export: Name: !Sub ${AWS::StackName}-NovaCustomizationJob "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Minh Duong\nPhone Number: 0347622638\nEmail: leduong5469@gmail.com\nUniversity: FPT University\nMajor: Information security\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Create an AWS account. Watch video tutorials on installing Hugo, Git, VScode. Learn how to write markdown, learn how to use hugo. Do labs on the First Cloud Journey Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Get acquainted with FCJ membersRead and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 Learn about AWS and its types of services:ComputeStorageNetworkingDatabase\u0026hellip; 09/09/2025 09/09/2025 CloudJourney 4 Create AWS Free Tier accountLearn about AWS Console \u0026amp; AWS CLIPractice:Create AWS accountInstall \u0026amp; configure AWS CLIHow to use AWS CLI 10/09/2025 10/09/2025 AWS Account Setup 5 Learn how to write markdown and use Hugo 11/09/2025 11/09/2025 YouTube Guide 6 Practice:Do labs on the First Cloud Journey 12/09/2025 12/09/2025 First Cloud Journey Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region successfully installed hugo and git and VScode:\nknow how to use hugo and git and VScode know how to write markdown and write a successful worklog week 1.\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": "Enabling Observability-First Operations at ScaleEnhance Data Visibility with Cribl Search and Amazon Managed Grafana by Rizwan Mushtaq, Kamilo \u0026ldquo;Kam\u0026rdquo; Amir, Sunil Ramachandra, and Aswin Vasudevan on 12 SEP 2025\nObservability is no longer a “nice to have” — it is a prerequisite to ensure systems operate securely, efficiently, and at scale. With the growing volume of logs, metrics, and security events, collecting, processing, and visualizing data in an organized way has become a challenge for many organizations. AWS, in partnership with Cribl Search, provides a centralized, flexible, and customizable observability pipeline — transforming raw data into actionable insights to drive faster, more confident decision-making at scale.\nArchitecture Guidance Below are the main components and how the integration between Cribl Search and Amazon Managed Grafana works:\ncomponent Main role Cribl Search Enables “in-place” search across multiple data sources ( Amazon S3, Cribl Lake, Amazon Security Lake, and AWS native services).\nNo need to index all the data before searching. Amazon Managed Grafana A powerful visualization tool: dashboards, real-time monitoring, and drill-down metrics/logs for troubleshooting. Integrated with a plugin for Cribl Search to query data directly from Cribl. Key Use Cases Some scenarios where this solution is particularly useful:\nCloud Infrastructure Monitoring\nYou can query data from AWS services via API or at rest, use Cribl to filter the required events, and then send them to a SIEM system or directly to Grafana dashboards to monitor health, resource performance, and costs.\nApplication Performance Management (APM)\nDashboards display latency, error rates, and user experience, with the ability to drill down into specific transactions. Grafana makes it clear and quick to identify application issues that need to be addressed.\nOperations \u0026amp; Security (SecOps)\nSecurity events are displayed in dedicated dashboards; Cribl supports continuous monitoring, compliance reporting, threat detection, and investigation workflows — helping SOC teams respond quickly to incidents.\nImplementation Walkthrough To set up and run the solution, the basic steps are as follows:\nPreparation An AWS account with administrative privileges. An S3 bucket (e.g., to store VPC Flow Logs) configured properly so services can write to it. A Cribl Cloud account and the necessary IAM/credentials. Authentication Setup (API Auth) Create a token / API credentials in Cribl so that Grafana can connect securely. Install Cribl Search Plugin in Grafana Go to the Plugins section in Amazon Managed Grafana, find the “Cribl Search” plugin, and add the connection using Cribl credentials. Create Dashboards \u0026amp; Visualizations Example: view VPC Flow Logs from the last 15 minutes, grouped by log status per minute. Switch to table view to analyze specific logs, detect anomalies, and trace request paths. Cleanup \u0026amp; Cost Management Delete unused resources: S3 bucket, Flow Log configurations, and temporary Cribl settings to avoid unnecessary costs. Benefits \u0026amp; Challenges Benefits:\nReduce unnecessary storage/routing costs since only meaningful data is processed in-depth.\nFaster incident response \u0026amp; analysis with real-time dashboards and drill-down logs.\nBetter control \u0026amp; compliance: all data and security events can be monitored.\nHigh flexibility: users can freely choose data sources, customize visualizations, and define routing needs.\nChallenges:\nProper security setup (credentials, access rights) is required to prevent data leaks or unauthorized access.\nData routing must be managed to avoid sending too much raw data to Grafana or SIEM.\nPipeline monitoring is necessary to ensure no misrouted, duplicated, or dropped messages.\nNew Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: CriblSearchDatasourceARN: Value: !Ref CriblSearchDatasource Export: Name: !Sub ${AWS::StackName}-CriblSearchDatasource GrafanaWorkspaceId: Value: !Ref GrafanaWorkspace Export: Name: !Sub ${AWS::StackName}-GrafanaWorkspaceId VPCFlowLogsBucket: Value: !Ref VPCFlowLogsBucket Export: Name: !Sub ${AWS::StackName}-VPCFlowLogsBucket "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, “Getting Started with Healthcare Data Lakes: Diving into Amazon Cognito”, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the “pub/sub hub.”\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function → ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda “trigger” subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 → JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Workshop Report: \u0026ldquo;DevOps on AWS\u0026rdquo; Event Objectives Get a clear understanding of DevOps culture, principles, and how it changes the way teams work Learn how to build CI/CD pipelines using AWS DevOps services Practice Infrastructure as Code with AWS CloudFormation and the AWS CDK Explore container platforms on AWS: ECR, ECS, EKS, and App Runner Set up monitoring and observability with Amazon CloudWatch and AWS X-Ray See how DevOps practices are applied in real production environments Event Details Location: AWS Vietnam Office Date \u0026amp; Time: 8:30 AM – 5:00 PM, Monday, November 17, 2025 Speakers \u0026amp; Facilitators Instructors:\nTruong Quang Tinh – AWS Community Builder DevOps culture and CI/CD fundamentals Van Hoang Kha – AWS Community Builder Infrastructure as Code with CloudFormation Nguyen Khanh Phuc Thinh – AWS Community Builder AWS CDK deep dive Le Huynh Nghiem – AWS Community Builder Container services on AWS Huynh Hoang Long – AWS Community Builder Monitoring and observability Pham Hoang Quy – AWS Community Builder DevOps best practices and real-world stories Facilitators:\nAWS Vietnam Team AWS Community Builders Vietnam Event Agenda 8:30 AM – 9:00 AM: Registration and Opening Check-in and informal networking Welcome talk and overview of the workshop flow Short introduction to key AWS DevOps services 9:00 AM – 10:30 AM: DevOps Culture and CI/CD Pipeline Speaker: Truong Quang Tinh\nDevOps Mindset – shifting from “Dev vs Ops” to “one team”:\nCollaboration: Dev and Ops work together instead of separately Automation: Let tools handle repetitive work so teams can focus on value Continuous Improvement: Use feedback to adjust and improve over time Shared Ownership: Quality, security, and reliability are everyone’s job DORA Metrics – how to know if DevOps is working:\nDeployment Frequency – how often you push changes live Lead Time for Changes – time from commit to production Mean Time to Recovery (MTTR) – how fast you recover from incidents Change Failure Rate – how many releases cause problems Core CI/CD Services on AWS:\nAWS CodeCommit – Git repositories hosted on AWS AWS CodeBuild – build and test automation service AWS CodeDeploy – roll out changes to EC2, Lambda, ECS, or on-prem servers AWS CodePipeline – main pipeline that ties all stages together Deployment Strategies:\nBlue/Green – switch traffic between two environments to reduce downtime Canary – release changes to a small portion of users first Rolling – update instances gradually so the service stays available Live Demo:\nSet up a basic CI/CD pipeline end-to-end using AWS DevOps tools 10:30 AM – 10:45 AM: Coffee Break Short break for coffee and networking 10:45 AM – 12:00 PM: Infrastructure as Code with CloudFormation Speaker: Van Hoang Kha\nCore Ideas of Infrastructure as Code (IaC):\nVersion Control – treat infrastructure like code stored in Git Repeatability – spin up the same environment again and again Living Documentation – templates describe what exists in the cloud Pre-deployment Testing – validate before roll-out CloudFormation Basics:\nTemplates – YAML/JSON files that describe AWS resources Stacks – a group of resources created and managed together Change Sets – preview what will change before you apply it Drift Detection – detect manual changes that differ from the template Best Practices with CloudFormation:\nModular Design – use nested stacks to reuse patterns Parameters – keep templates flexible with input values Outputs – expose useful values for other stacks or teams Cross-Stack References – connect multiple stacks cleanly Advanced Concepts:\nStack Policies – prevent accidental changes to critical pieces Rollback Triggers – roll back when CloudWatch alarms are triggered StackSets – roll out stacks to multiple accounts/regions at once Live Demo:\nDeploy a simple multi-tier app using CloudFormation templates 12:00 PM – 1:00 PM: Lunch Break Lunch and informal discussion with speakers and participants 1:00 PM – 2:15 PM: AWS CDK Deep Dive Speaker: Nguyen Khanh Phuc Thinh\nWhat is AWS CDK?\nInfrastructure described with real programming languages Supports TypeScript, Python, Java, C#, Go, and more Different abstraction layers: L1 – direct CloudFormation mapping L2 – higher-level constructs L3 – ready-made patterns CDK Building Blocks:\nConstructs – reusable building units for cloud apps Stacks – deployment units made of constructs Apps – a collection of stacks deployable as one project Synthesis – CDK code → CloudFormation template CDK vs CloudFormation:\nImperative vs Declarative – use loops, conditions, functions in CDK Type Safety – catch mistakes early via the compiler IDE Support – autocompletion and inline docs help productivity Testing – write tests for your infrastructure code CDK Good Practices:\nUse and share construct libraries Design stacks that are environment-agnostic when possible Keep logical IDs stable to avoid unnecessary replacements Use context values for per-environment configuration Live Demo:\nBuild and deploy a small serverless app using AWS CDK 2:15 PM – 2:30 PM: Coffee Break Short break and more networking 2:30 PM – 3:45 PM: Container Services on AWS Speaker: Le Huynh Nghiem\nContainer Basics:\nWhat containers are and why they’re popular Benefits: consistent environments, portability, resource efficiency Quick review of Docker concepts: images, containers, registries Amazon Elastic Container Registry (ECR):\nPrivate container image registry managed by AWS Built-in image scanning for vulnerabilities Lifecycle policies to auto-delete old images Cross-region replication to speed up deployments globally Amazon Elastic Container Service (ECS):\nAWS-native container orchestration service Task Definitions – recipe for running containers Services – define desired count and keep tasks running Fargate – run containers without managing servers EC2 Launch Type – full control of the underlying instances Amazon Elastic Kubernetes Service (EKS):\nManaged control plane for Kubernetes clusters Use standard Kubernetes APIs and tools Flexible enough for complex microservice architectures Integration with AWS components like ALB, EBS, EFS AWS App Runner:\nSimplest way to go from source code or container image to a running service Handles scaling automatically Integrates with CI/CD flows Ideal for web apps and APIs that don’t need complex orchestration Choosing the Right Container Service:\nECS – good default choice for AWS-focused teams EKS – for teams already familiar with Kubernetes App Runner – when speed and simplicity are top priority Live Demo:\nDeploy a container-based application to ECS and EKS 3:45 PM – 4:45 PM: Monitoring and Observability Speaker: Huynh Hoang Long\nMonitoring vs Observability:\nMonitoring – collecting metrics and checking if things are OK Observability – being able to understand internal behavior from external outputs Focus on the three pillars: logs, metrics, traces Amazon CloudWatch:\nMetrics – track CPU, memory (custom), and app metrics Logs – central place to store and search logs Alarms – trigger alerts or actions based on thresholds Dashboards – custom views for system health Logs Insights – query logs with a SQL-like syntax Events – react to changes in your AWS environment AWS X-Ray:\nDistributed tracing – follow a request across multiple services Service map – visual view of how services talk to each other Trace analysis – identify slow parts and bottlenecks Error analysis – pinpoint where failures happen Works with Lambda, ECS, EKS, API Gateway, and more Good Practices for Observability:\nUse structured logging (JSON or consistent formats) Add custom metrics for business-level KPIs Configure alerts before users notice issues Correlate logs, metrics, and traces for faster debugging Apply retention policies to balance visibility and cost Live Demo:\nSet up dashboards, alarms and tracing for a small microservices system 4:45 PM – 5:00 PM: DevOps Best Practices \u0026amp; Q\u0026amp;A Speaker: Pham Hoang Quy\nDevOps Best Practices:\nAutomate as much as possible: tests, builds, deployments, checks Fail fast and learn: treat incidents as learning opportunities Blameless postmortems: focus on process, not on blaming people Progressive delivery: feature flags, canary, blue/green releases Continuous learning: DevOps is an ongoing improvement cycle Case Studies:\nStartup – use serverless and CI/CD to deliver features quickly Enterprise – multi-account setups with CloudFormation StackSets E-commerce – high-availability deployments using blue/green Q\u0026amp;A Session:\nOpen discussion with participants about tools, culture, and real problems Key Takeaways DevOps Culture and Mindset DevOps is about people, collaboration, and shared goals, not just tools Automation is key to reducing manual work and mistakes DORA metrics give a concrete way to see how the team is improving Continuous improvement is at the heart of DevOps Quality and reliability are shared responsibilities across the team CI/CD Pipeline Automation CodeCommit, CodeBuild, CodeDeploy, and CodePipeline work well together End-to-end automation improves speed and reduces human error Multiple deployment patterns (blue/green, canary, rolling) support safer releases CI/CD can integrate with both AWS-native and third-party services A good pipeline becomes the backbone of the delivery process Infrastructure as Code CloudFormation templates provide a clear and repeatable infrastructure definition CDK adds a more developer-friendly way to define the same resources IaC improves consistency, traceability, and collaboration Drift detection helps catch manual edits that break the IaC model Choosing between CloudFormation and CDK depends on team skills and preference Container Services ECR secures and manages container images with scanning and policies ECS is a strong default for many AWS-based container workloads EKS is powerful for teams already invested in Kubernetes App Runner is ideal when you want “code → service” with minimal steps The right container platform depends on complexity, control, and team experience Monitoring and Observability CloudWatch and X-Ray provide a complete toolkit for visibility on AWS Observability helps understand why something went wrong, not just that it did Proactive alerts are essential for minimizing downtime Logs, metrics, and traces together give a full picture of system health Data from monitoring can guide performance tuning and cost optimizations DevOps Best Practices Automation + culture = effective DevOps, tools alone are not enough Learning from incidents is more important than avoiding them at all costs Progressive delivery reduces risk for every release Regularly reviewing processes and metrics keeps the team moving forward Applying to Work Start with a simple CI/CD pipeline using CodePipeline and expand from there Migrate manual setups into CloudFormation or CDK for consistency Gradually containerize applications and choose ECS/EKS/App Runner where appropriate Strengthen monitoring and observability with CloudWatch dashboards and X-Ray Encourage a DevOps culture of transparency, ownership, and continuous learning Use DORA metrics to track improvement instead of gut feeling Consider aiming for the AWS DevOps Engineer certification as a longer-term goal Event Experience The “DevOps on AWS Workshop” was a full-day, very packed but valuable experience. It covered not only the technical tools, but also the mindset and real-world practices behind DevOps.\nLearning from AWS Experts The speakers provided both high-level concepts and concrete examples Live demos made the theory easier to follow and remember Real case studies helped connect the tools to realistic use cases There was plenty of time to ask specific questions about real problems Hands-on Demonstrations Saw a complete CI/CD pipeline being assembled step by step Watched how IaC with CloudFormation and CDK works in practice Learned how to deploy containers on ECS and EKS Understood what proper monitoring and tracing look like in a microservices setup Understanding DevOps in Practice DevOps was presented as a combination of culture + process + tools DORA metrics provided a clear way to talk about performance and improvement Observability was highlighted as a core requirement for modern systems Examples of incident handling and postmortems were particularly useful Networking and Community Met many engineers who are on the same DevOps journey Shared common struggles like legacy systems, manual processes, and resistance to change Got connected to the AWS DevOps community for future events and learning Personal Takeaways Good DevOps requires both mindset change and technical skills Automation frees teams from “busy work” and lets them focus on real problems Having IaC and proper monitoring in place makes scaling much easier AWS provides almost everything needed to implement DevOps end-to-end Next Steps Build or improve an internal CI/CD pipeline using the ideas from the workshop Refactor existing infrastructure into CloudFormation/CDK where possible Plan a gradual move towards containers for suitable workloads Invest time in setting up meaningful dashboards and alerts Keep learning from AWS documentation, workshops, and the community "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Finish Module 01 of the First Cloud Journey. Understand the basics of cloud, AWS, and account management. Learn how AWS cost optimization works. Complete all Module 01 labs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Review cloud basics: Module 01-01 → 01-03Note down benefits of using AWS 15/09/2025 15/09/2025 FCJ Module 01-01 → 01-03 3 Learn about AWS Global InfrastructurePractice using AWS Console and service search 16/09/2025 16/09/2025 Module 01-04, 01-05 4 Study AWS cost optimizationDo cost labs: Lab07-01 → Lab07-06 17/09/2025 17/09/2025 Module 01-06 + Lab07 5 Account-related labs:Create AWS accountEnable MFACreate admin user + admin groupTest account authentication 18/09/2025 18/09/2025 Lab01-01 → Lab01-04 6 Learn about AWS Support packagesTry creating a test support caseReview how to track and close cases 19/09/2025 19/09/2025 Lab09-01 → Lab09-04 Week 2 Achievements: Understand how cloud computing works and why AWS is beneficial. Learned about AWS Regions, AZs, and Edge Locations. Completed AWS Budgets labs (cost budget, usage budget, Savings Plans). Finished account labs: admin group, MFA, IAM user creation, authentication workflow. Know how AWS Support plans work and how to open a support case. Finished the entire Module 01 and ready to move to Networking next week. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Security Scan Pipeline on AWS Cloud Automated solution for source code security analysis in the DevSecOps pipeline 1. BACKGROUND AND PROJECT DRIVERS 1.1 EXECUTIVE SUMMARY The client is developing an application and wants to adopt a DevSecOps model to automate the build–scan–deploy workflow, improve security, and shorten release cycles.\nSystem objectives:\nDetect security vulnerabilities and code smells early in the source code using Amazon CodeGuru Reviewer. Automatically build and create artifacts when new commits occur. Store artifacts securely in Amazon S3. Automatically deploy the application to Amazon EC2 via AWS CodeDeploy. Use cases:\nAutomatic security scanning on new commits. Automated build and artifact creation. Secure artifact storage. Automatic EC2 deployment. Centralized alerts and process monitoring. Professional services provided:\nImplement CodePipeline integrated with GitHub, include CodeBuild + CodeGuru Reviewer, configure S3 artifact storage, deploy to EC2 with CodeDeploy, and enable security monitoring and logging (CloudWatch, CloudTrail, GuardDuty, Security Hub). 1.2 PROJECT SUCCESS CRITERIA The client provides a GitHub repository with appropriate permissions. IAM Roles have sufficient permissions for CodeBuild, CodeDeploy, S3, and CloudWatch. EC2 instances are provisioned and ready for deployment. The application supports deployment via appspec.yml. Multiple environments (dev/stg/prod) are not required at this time. AWS Free Tier is used to reduce costs where possible. Potential risks: timeouts, webhook failures, corrupted artifacts, insufficient IAM permissions. 1.3 ASSUMPTIONS The customer provides a GitHub repository with appropriate permissions. IAM Roles have sufficient permissions for CodeBuild, CodeDeploy, S3, and CloudWatch. EC2 instances are valid and ready for deployment. The application supports deployment via appspec.yml. Multiple environments (dev/stg/prod) are not required at this time. AWS Free Tier is used to minimize costs. Potential risks include: timeouts, webhook failures, corrupted artifacts, insufficient IAM permissions. 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM Architecture diagram: AWS services used:\nAWS CodePipeline AWS CodeBuild Amazon CodeGuru Reviewer Amazon S3 AWS CodeDeploy Amazon EC2 Amazon SNS CloudWatch, CloudTrail, GuardDuty, Detective, Security Hub 2.2 TECHNICAL PLAN Includes:\nConfigure GitHub connection (OIDC or Personal Access Token). Create CodeBuild project and provide buildspec.yml. Integrate Amazon CodeGuru Reviewer. Configure CodeDeploy and appspec.yml. Enable monitoring and security logging following AWS Well-Architected best practices. 2.3 PROJECT PLAN We will follow Agile Scrum with two sprints (2 weeks per sprint).\nTeam responsibilities:\nDevOps Engineer: design and implement the pipeline Developer: support testing and provide the repository Security Analyst: review alerts from CodeGuru \u0026amp; GuardDuty Meeting cadence:\nDaily Standup Weekly Review Bi-weekly Retrospective Knowledge transfer: one training session on using the pipeline and one session on viewing logs \u0026amp; reports.\n2.4 SECURITY CONSIDERATIONS Access Enable MFA Use IAM Roles instead of long-term Access Keys Infrastructure Security Groups with IP restrictions Data S3 encryption (SSE-S3) Detection GuardDuty, Detective, CloudTrail Incident Management SNS alerts for failed builds/deployments 3. ACTIVITIES \u0026amp; DELIVERABLES 3.1 ACTIVITIES \u0026amp; DELIVERABLES Phase Duration Activity Deliverable Man-day Assessment Week 1 Requirements gathering Initial architecture X Infrastructure setup Week 1–2 S3, IAM, EC2, GitHub Infrastructure X Pipeline setup Week 2–3 CodePipeline + Build + Scan Complete pipeline X Deployment setup Week 3 CodeDeploy Automated deploy X Testing \u0026amp; Go-live Week 4 Test the pipeline Test report X Handover Week 4 Training + documentation Final handover X 3.2 OUT OF SCOPE No additional staging/production environments will be provisioned. No integration with SonarQube / Trivy / Checkov. No advanced dashboard design. No autoscaling or load balancer configuration. 3.3 PATH TO PRODUCTION The POC is intended for the initial demo only. To move to production, additional work is required: automated unit tests, enhanced monitoring, detailed error handling, and multi-environment CI/CD.\n4. AWS COST ESTIMATE Estimated monthly costs:\nCodePipeline: $0.40 / month CodeBuild: $0.35 / month S3: $0.10 / month CodeDeploy: $0.20 / month EC2 t2.micro: $0.10 / month CloudWatch + SNS: $0.05 / month Total: ~ $1.2 / month (~ $14.4 / year)\n5. PROJECT TEAM Executive Sponsor \u0026amp; Stakeholders\nTeam: First Cloud Journey\nProject Team\nName Student ID Email / Contact Lê Công Cảnh SE183750 canhlcse183750@fpt.edu.vn Phùng Gia Đức SE183187 ducpgse183187@fpt.edu.vn Vũ Nguyễn Bình SE193185 vunguyenbinh25@gmail.com Lê Minh Dương SE184079 duonglmse184079@fpt.edu.vn Nguyễn Phi Duy SE180529 duynpse180529@fpt.edu.vn 6. RESOURCES \u0026amp; COST ESTIMATES See the cost estimate on the AWS Pricing Calculator or download the budget estimation file.\n6.1 Resource Allocation \u0026amp; Hourly Rates Resource Responsibility Rate (USD) / Hour Headcount Solution Architects Architecture design, security review, AWS service integration, oversight of CI/CD \u0026amp; scanning pipeline 6 – 9 1 Engineers (DevOps / Cloud) Implement CI/CD pipeline, configure CodePipeline/CodeBuild/CodeDeploy, IAM setup, testing, documentation 4 – 7 1–2 Other (Security Engineer) Integrate code scanning tools (SonarQube, Trivy, CodeGuru Security), analyze reports 5 – 8 1 6.2 Estimated Project Hours by Phase Project Phase Solution Architects (Hours) Engineers (Hours) Other (Hours) Total Hours Phase 1 – Discovery \u0026amp; Requirements 4 4 0 8 Phase 2 – Architecture Design 6 2 0 8 Phase 3 – Pipeline Implementation 4 20 6 30 Phase 4 – Security Scanning Integration 2 6 8 16 Phase 5 – Testing \u0026amp; Validation 2 8 4 14 Phase 6 – Documentation \u0026amp; Handover 2 6 2 10 Total Hours 20 46 20 86 Hours 7. ACCEPTANCE The client has 8 days to review the deliverables. If no response is received, the deliverables will be considered accepted. If defects are found, the provider will fix and resubmit according to the Rejection Notice process. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Well-Architected Security Pillar Workshop\u0026rdquo; Event Details Date: November 29, 2025 — Morning Only Time: 08:30 AM – 12:00 PM Location: AWS Vietnam Office Speakers Le Vu Xuan An – AWS Cloud Club Captain HCMUTE Tran Duc Anh – AWS Cloud Club Captain SGU Tran Doan Cong Ly – AWS Cloud Club Captain PTIT Danh Hoang Hieu Nghi – AWS Cloud Club Captain HUFLIT 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Role of the Security Pillar within the Well-Architected Framework Core principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility Model Common cloud threats in Vietnam Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture IAM basics: Users, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO and permission sets SCPs and permission boundaries for multi-account setups MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring Organization-level CloudTrail, GuardDuty, Security Hub Logging across all layers: VPC Flow Logs, ALB Logs, S3 Access Logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules as code) 9:55 – 10:10 AM | Coffee Break Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security VPC segmentation: private vs public placement Security Groups vs NACLs and when to use each WAF, Shield, Network Firewall Workload protection fundamentals: EC2, ECS/EKS Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets KMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit for S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store best practices Data classification and access guardrails Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation AWS Incident Response lifecycle Example playbooks: Compromised IAM key Public S3 exposure EC2 malware detection Snapshotting, isolation \u0026amp; evidence collection Automated response with Lambda / Step Functions 11:40 AM – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of all 5 pillars Common pitfalls \u0026amp; real examples from Vietnamese companies Recommended learning path (Security Specialty, SA Pro) "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 3 Objectives: Start Module 02 – Networking. Understand VPC, subnet, route table, IGW, NAT Gateway. Know the difference between Security Groups and NACLs. Complete the basic VPC labs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Watch Module 02-01: VPC basicsLearn CIDR, subnets, public/private subnet 22/09/2025 22/09/2025 Module 02-01 3 Module 02-02: SG vs NACLNote down stateful vs stateless behavior 23/09/2025 23/09/2025 Module 02-02 4 Module 02-03: VPN, Direct Connect, Load Balancer overview 24/09/2025 24/09/2025 Module 02-03 5 Start VPC labs:Lab03-01: Create VPCLab03-01.1: SubnetsLab03-01.2: Route TablesLab03-01.3: Internet GatewayLab03-01.4: NAT Gateway 25/09/2025 25/09/2025 Lab03-01 → 01.4 6 Continue with security labs:Lab03-02.1: Security GroupLab03-02.2: NACLLab03-02.3: Resource Map 26/09/2025 26/09/2025 Lab03-02.x Week 3 Achievements: Understood the basic blocks of AWS networking. Created VPC, subnets, routers, IGW, and NAT Gateway. Learned the difference between SG and NACL clearly. Finished all basic VPC labs for Module 02. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs you have translated. For example:\nBlog 1 -Enabling customers to deliver production-ready AI agents at scale This blog introduces how AWS helps organizations deliver production-ready AI agents at scale using a microservices architecture. You will learn why microservices are critical for deploying AI agents in real-world environments, how they improve flexibility, scalability, and maintainability, and how AWS services such as AgentCore, Amazon Nova customization, and supporting components form a modular foundation. The article also explores the architecture principles, communication patterns, and customer adoption stories that demonstrate how enterprises can shift from experimentation to large-scale production.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices…), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Continue Module 02 labs. Launch EC2 instances inside VPC subnets. Practice Hybrid DNS with Route 53 Resolver. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Lab03-03.x: Build full VPC setup 29/09/2025 29/09/2025 Lab03-03 3 Lab03-04.x: Launch EC2, test internal/external traffic 30/09/2025 30/09/2025 Lab03-04 4 Learn about hybrid DNS and Route53 Resolver 01/10/2025 01/10/2025 Module 02 5 Lab10-01 → Lab10-03: RDGW + DNS setup 02/10/2025 02/10/2025 Lab10 6 Lab10-05.x: Outbound/Inbound resolverDNS testing + cleanup 03/10/2025 03/10/2025 Lab10 Week 4 Achievements: Built a full VPC networking environment. Launched EC2 in both public and private subnets. Worked with Route53 Resolver for hybrid DNS. Completed all remaining Module 02 basics. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:30 AM – 12:00 PM, Saturday, November 15, 2025\nLocation: 36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 8:30 AM – 5:00 PM, Monday, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Well-Architected Security Pillar Workshop Date \u0026amp; Time: 08:30 AM – 12:00 PM, November 29, 2025 — Morning Only\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Learn the difference between VPC Peering and Transit Gateway. Practice connecting multiple VPCs. Complete Module 02 advanced labs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Module intro + Lab19-01 → 19-03 06/10/2025 06/10/2025 Lab19 3 Lab19-04: Create peering connectionLab19-05: Update route tables 07/10/2025 07/10/2025 Lab19 4 Lab19-06: Cross-peer DNSCleanup 08/10/2025 08/10/2025 Lab19 5 Start Transit Gateway labs (Lab20-01 → 20-03) 09/10/2025 09/10/2025 Lab20 6 Lab20-04 → 20-07: Attachments + routing 10/10/2025 10/10/2025 Lab20 Week 5 Achievements: Learned how to connect VPCs using peering and TGW. Successfully tested traffic between VPCs. Completed all advanced networking labs. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": " ⚠️ Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nSecure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Start Module 03 – Compute. Learn EC2 basics: AMI, Instance Type, EBS. Do backup \u0026amp; Storage Gateway labs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Watch EC2 introduction 13/10/2025 13/10/2025 Module 03 3 Lean about AMI, Instance Types, EBS volumes 14/10/2025 14/10/2025 03-01-01 → 03-01-03 4 Practice User Data + Metadata 15/10/2025 15/10/2025 03-01-05 → 03-01-06 5 Backup labs: Lab13-01 → Lab13-06 16/10/2025 16/10/2025 Lab13 6 Storage Gateway + S3 labs: Lab24 + Lab57 17/10/2025 17/10/2025 Lab24, Lab57 Week 6 Achievements: Launched EC2, created AMI and snapshots. Completed backup \u0026amp; restore labs. Understood how EBS, metadata, and User Data work. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at [Company/Organization Name] from [start date] to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in [briefly describe the main project or task], through which I improved my skills in [list skills: programming, analysis, reporting, communication, etc.].\nIn terms of work ethic, I always strived to complete tasks well, complied with workplace regulations, and actively engaged with colleagues to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ☐ ✅ ☐ 2 Ability to learn Ability to absorb new knowledge and learn quickly ☐ ✅ ☐ 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ✅ ☐ ☐ 4 Sense of responsibility Completing tasks on time and ensuring quality ✅ ☐ ☐ 5 Discipline Adhering to schedules, rules, and work processes ☐ ☐ ✅ 6 Progressive mindset Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in teams ✅ ☐ ☐ 9 Professional conduct Respecting colleagues, partners, and the work environment ✅ ☐ ☐ 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ☐ ✅ ☐ 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ✅ ☐ ☐ 12 Overall General evaluation of the entire internship period ☐ ✅ ☐ Needs Improvement Strengthen discipline and strictly comply with the rules and regulations of the company or any organization Improve problem-solving thinking Enhance communication skills in both daily interactions and professional contexts, including handling situations effectively "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn everything about S3. Build a static website on S3. Practice CloudFront, versioning, replication. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Study S3 access control 20/10/2025 20/10/2025 Module 04 3 Learn static hosting + CORS 21/10/2025 21/10/2025 Module 04 4 Lab57-02 → 07.3: S3 + CloudFront 22/10/2025 22/10/2025 Lab57 5 Versioning + Replication labs 23/10/2025 23/10/2025 Lab57-08 → 11 6 Finish Module 04 labs 24/10/2025 24/10/2025 Module 04 Labs Week 7 Achievements: Successfully hosted a website on S3. Configured CloudFront distribution. Learned versioning, object movement, and multi-region replication. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Understand AWS security fundamentals. Practice IAM: users, roles, policies. Work with Organizations, Identity Center, and KMS. Complete major security labs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Learn IAM basics 27/10/2025 27/10/2025 Module 05 3 Role, policy, STS 28/10/2025 28/10/2025 IAM 4 Organizations + Identity Center 29/10/2025 29/10/2025 AWS Org 5 KMS basics + key creation 30/10/2025 30/10/2025 KMS 6 Security labs: Lab18, 22, 27, 28, 30, 33 31/10/2025 31/10/2025 Module 05 Labs Week 8 Achievements: Know how AWS security model works. Created users, roles, and policies. Used KMS for encryption. Completed the main security labs. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Learn RDS, Aurora, DynamoDB, ElastiCache. Practice building databases on AWS. Do Migration Service labs. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Database concepts 03/11/2025 03/11/2025 Module 06 3 RDS + Aurora theory 04/11/2025 04/11/2025 Module 06 4 Lab05 VPC \u0026amp; SG setup 05/11/2025 05/11/2025 Lab05 5 Create RDS + connect EC2 06/11/2025 06/11/2025 Lab05 6 Migration labs: Lab43 07/11/2025 07/11/2025 Lab43 Week 9 Achievements: Created RDS and connected it to EC2. Understood DB subnet groups, SGs, and backup/restore. Completed migration labs from MSSQL/Oracle to Aurora. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Work with AWS analytics services. Practice Glue, Athena, Kinesis, and QuickSight. Complete labs 35, 39, and 40. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Glue Crawler \u0026amp; Data Catalog 10/11/2025 10/11/2025 Lab35 3 Kinesis Data Streams 11/11/2025 11/11/2025 Lab35 4 Athena queries 12/11/2025 12/11/2025 Lab35 5 DynamoDB labs 13/11/2025 13/11/2025 Lab39 6 Cost allocation \u0026amp; tagging 14/11/2025 14/11/2025 Lab40 Week 10 Achievements: Built a basic data pipeline. Learned how Glue, Athena, and QuickSight fit together. Used tags for AWS cost tracking. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Learn CloudShell, SDK usage, and Cloud9. Practice ETL with Glue \u0026amp; DataBrew. Work on end-to-end data pipeline (Lab72). Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 CloudShell basics + SDK 17/11/2025 17/11/2025 Lab60 3 Set up Cloud9 environment 18/11/2025 18/11/2025 Lab70 4 Clean data with DataBrew 19/11/2025 19/11/2025 Lab70 5 Start the big Lab72 pipeline 20/11/2025 20/11/2025 Lab72 6 Glue, EMR, Athena, Kinesis workflows 21/11/2025 21/11/2025 Lab72 Week 11 Achievements: Used CloudShell and Cloud9 for development. Practiced data cleaning with DataBrew. Built a multi-step pipeline using Glue, EMR, and Athena. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Finish all remaining labs (Module 72 + 73). Clean up all AWS resources to avoid charges. Summarize everything learned in the Bootcamp. Complete final internship report. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Finish Glue advanced labs 24/11/2025 24/11/2025 Lab72 3 Build dashboard with QuickSight 25/11/2025 25/11/2025 Lab73 4 Clean AWS resources 26/11/2025 26/11/2025 AWS Console 5 Write summary notes 27/11/2025 27/11/2025 — 6 Complete the internship final report 28/11/2025 28/11/2025 — Week 12 Achievements: Completed all Bootcamp modules and labs. Fully cleaned AWS resources to avoid extra charges. Summarized key learnings from cloud, networking, compute, storage, security, database, and analytics. Ready to submit the final internship report. "
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://minhDuong27.github.io/FCJ-WORKSHOP-REPORT/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]