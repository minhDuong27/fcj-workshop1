[
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Workshop Report: \u0026ldquo;AI/ML/GenAI on AWS\u0026rdquo; Workshop Objectives Gain an overview of the AI/ML landscape and how AWS is adopted in Vietnam Understand the full ML workflow using Amazon SageMaker Explore Generative AI capabilities through Amazon Bedrock Practice prompt engineering and RAG (Retrieval-Augmented Generation) Learn how to build practical AI/ML solutions using AWS services Event Information Location: AWS Vietnam Office Time: 8:30 AM – 12:00 PM, Saturday, November 15, 2025 Speakers \u0026amp; Organizing Team Instructors:\nLâm Tuấn Kiệt – Senior DevOps Engineer, FPT Software Presented the SageMaker overview and general AWS ML services Đinh Lê Hoàng Anh – Cloud Engineer Trainee, FCAJ, Swinburne University of Technology Covered Amazon Bedrock and AWS AI/ML service ecosystem Danh Hoàng Hiếu Nghị – Fresher AI Engineer, Renova Cloud Demonstrated Bedrock Agent Core and guided the hands-on practice Coordinators:\nAWS Vietnam Community Team FCJ (First Cloud Journey) Program Leads Agenda Overview 8:30 – 9:00 AM: Registration \u0026amp; Kickoff Participant check-in and early networking Introduction to the workshop structure and expected outcomes Ice-breaker session A quick look at the AI/ML market and adoption trends in Vietnam 9:00 – 10:30 AM: AWS AI/ML Overview – Deep Dive into Amazon SageMaker Amazon SageMaker – The End-to-End ML Platform\nData Preparation \u0026amp; Labeling:\nData Wrangler for cleaning and transforming datasets Ground Truth for labeling and annotation Feature Store for centralizing and reusing features Training, Tuning \u0026amp; Deployment:\nUse built-in algorithms or bring your own training scripts Run hyperparameter tuning jobs Deploy models via real-time, batch, or serverless inference Experiment with A/B testing models and multi-model endpoints Built-in MLOps Capabilities:\nSageMaker Pipelines to automate ML workflows Model Registry for tracking versions and governance Model Monitor to detect drift and quality issues CI/CD integration for continuous deployment Live Demo – SageMaker Studio:\nLaunch a notebook instance Train a sample ML model Deploy an endpoint and send test requests 10:30 – 10:45 AM: Coffee Break Short break for refreshments Casual Q\u0026amp;A with AWS engineers 10:45 AM – 12:00 PM: Generative AI with Amazon Bedrock \u0026amp; AWS AI/ML Services AWS AI/ML Services Overview\nRekognition – Image/video analysis Translate – Neural machine translation Textract – Document text extraction Transcribe – Speech-to-text Polly – Natural-sounding text-to-speech Comprehend – NLP and text analytics Kendra – Intelligent search engine Lookout – Industrial anomaly detection Personalize – ML-powered recommendations Foundation Models: Claude, Llama, Titan\nModel Selection Insights: Claude: Strong in reasoning-heavy conversation tasks Llama: Good for customization, open-source flexibility Titan: Amazon-native, cost-effective, integrated with AWS How to choose depending on use case Prompt Engineering Essentials\nCore Prompting Techniques:\nProvide clear instructions and meaningful context Use few-shot examples Apply chain-of-thought for complex logic Use role-based prompting to guide model behavior Advanced Prompting:\nTuning temperature and token limits Knowing when to use system vs user prompts Template-based prompting for reusability Retrieval-Augmented Generation (RAG)\nRAG Architecture:\nEmbeddings + vector search Semantic retrieval before generating answers Feeding retrieved context back into prompts Knowledge Base Integration:\nBedrock Knowledge Bases Store documents in Amazon S3 Connect to data sources (S3, DBs, external APIs) Best practices for chunking and metadata Set correct bucket policies for secure access Amazon Bedrock Agent Core\nBuilding Autonomous Agents:\nMulti-step task planning and orchestration Action groups for calling APIs Persistent memory for context retention Tool Integration with Lambda:\nLambda as an execution engine for custom logic Real-time data processing Querying databases or external systems Serverless approach → simplified scaling and maintenance Guardrails for Safe AI\nContent filtering and moderation PII detection and redaction Topic restrictions and safety rules Custom guardrails for enterprise policies Live Demo – Creating a GenAI Chatbot with Bedrock\nEnabling foundation model access Designing prompts for a basic chatbot Adding RAG with Knowledge Bases Applying guardrails to control responses Testing and fine-tuning the bot Key Insights From Amazon SageMaker A complete platform that covers the entire ML lifecycle Well-integrated MLOps tools Easy scaling from experimentation to production Flexible pricing models to control cost From Amazon Bedrock Variety of foundation models readily available Prompt engineering plays a major role in result quality RAG helps integrate enterprise knowledge effectively Guardrails are essential for safe deployment Agent Core enables multi-step intelligent workflows Practical Lessons Start with the business problem, not the tool Prototype quickly using SageMaker Studio Use foundation models before investing in custom training Guardrails ensure safety and compliance Always monitor performance and optimize workloads Applying the Knowledge Experiment in SageMaker Studio using small datasets Build a RAG-based chatbot using Bedrock + S3 Practice prompt engineering with different models Automate ML pipelines with SageMaker Pipelines Try building simple Bedrock Agents for internal workflows Implement guardrails before deploying any AI app Share learnings with teammates to improve team-wide standards Personal Experience Attending the “AI/ML/GenAI on AWS Workshop” at the AWS Vietnam Office was a hands-on and insightful experience. The mix of explanations, demos, and real-world examples made the content easier to understand.\nLearning from AWS Experts Clear explanations on SageMaker’s end-to-end workflow Solid demos of Bedrock and real GenAI applications Many Vietnam-based use cases provided good context Practical advice on choosing the right AWS tools Hands-on Demonstrations Observed the full ML process: data → training → deployment Learned how Bedrock simplifies building GenAI applications Applied prompt engineering tricks in real examples Understood how RAG makes LLMs more accurate with enterprise data Saw how Agents organize multi-step tasks Understanding AI/ML Trends Differences between traditional ML and GenAI became clearer Better idea of when to pick SageMaker vs Bedrock Understood the importance of proper MLOps for production systems Networking Met many developers and data enthusiasts exploring AWS AI/ML Exchanged experiences about real-world implementation challenges Built new connections with AWS community members Important Takeaways Foundation models significantly reduce development effort Good prompts dramatically improve model output RAG is essential for knowledge-heavy chatbot applications Guardrails are not optional—they are required for safe AI SageMaker is ideal for long-term, scalable ML projects Next Steps Continue exploring SageMaker Studio hands-on Build a small RAG proof-of-concept using Bedrock Practice prompt engineering with multiple foundation models Experiment with Bedrock Agents for automation workflows Learn more about MLOps and monitoring practices Engage with the AWS AI/ML community for ongoing learning Event Pictures Overall, this workshop provided a comprehensive introduction to AWS AI/ML services, from traditional machine learning with SageMaker to cutting-edge Generative AI with Bedrock. The hands-on demonstrations and expert guidance made complex concepts accessible and immediately applicable. The key takeaway is that AWS provides a complete ecosystem for building, deploying, and scaling AI/ML applications, making it easier than ever to bring AI innovations to production.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/3-blogstranslated/3.1-blog1/",
	"title": "Using Large Language Models on Amazon Bedrock for Multi-step Task Execution",
	"tags": [],
	"description": "",
	"content": "This article explains how to use Large Language Models (LLMs) on Amazon Bedrock to carry out analytical tasks that require multi-step reasoning and external APIs. The goal is to convert complex questions into a structured Plan → Execute workflow that is predictable and scalable.\n1. Background Business analytics questions are typically complex, such as:\n“What is the average hospital stay for patients with a specific condition across hospitals?” “How do prescription trends for a particular medication differ across regions?” Traditional solutions require BI experts and data engineers.\nWith LLMs + tools, we can automate and accelerate these workflows.\n2. Tools in the LLM Context Tools are external capabilities the LLM can call, such as:\nAPIs for real-time data Functions for computation or data processing Filtering, grouping, or joining data Using tools allows the LLM to produce accurate, contextual, and actionable outputs.\n3. Example Interaction User: “Who is the patient with the lowest number of vaccines?”\nAI: “The patient is Sharleen176 Kulas532 with 1 vaccine.”\nSteps performed:\nRetrieve patient data Retrieve immunization data Group by patient Count vaccines Sort ascending Select top result Join with patient details 4. Dataset \u0026amp; Setup The system uses the Synthetic Patient Generation Dataset, containing multiple healthcare tables.\nSetup involves downloading, extracting, and placing data in the project folder.\n5. Solution Architecture: Plan → Execute Two-phase workflow:\nPlan: LLM generates a step-by-step plan Execute: Engine executes each step Flow:\nUser → LLM Plan → JSON Plan → Execution Engine → Answer\n6. Planning Phase Why Planning? The LLM:\nPerforms step-by-step reasoning Produces a structured workflow Avoids hallucinated API calls Tools are provided as function signatures the LLM can use.\nRAG for Tool Selection RAG helps show the LLM only relevant tools, improving accuracy and reducing complexity.\nExample Plan For the query “Find the patient with the fewest vaccines,” the plan includes:\nRetrieve patients Retrieve immunizations Group by patient Count Sort Limit Join Select 7. Execution Phase The engine takes the JSON plan and:\nParses it Executes each function Stores intermediate results Returns the final answer The LLM then frames the output into a natural-language response.\n8. Error Handling Possible failures:\nEmpty or missing data Invalid parameters Type mismatches Engine responsibilities:\nValidate inputs Validate outputs Return meaningful errors The LLM may regenerate a better plan automatically.\n9. Summary We explored:\nHow LLMs use APIs to answer complex questions The Plan → Execute architecture The role of RAG and function signatures Error handling in execution LLMs can now act as orchestration brains for analytical workflows.\n10. Future Improvements Extensions include:\nMore analytical questions Additional tool signatures A web UI for question input, plan visualization, and execution logs This makes the project both academically strong and practically relevant.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Le Minh Duong\nPhone Number: 0347622638\nEmail: leduong5469@gmail.com\nUniversity: FPT University\nMajor: Information security\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.1-week1/",
	"title": "Week 1 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Create an AWS account. Watch video tutorials on installing Hugo, Git, VScode. Learn how to write markdown, learn how to use hugo. Do labs on the First Cloud Journey Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 Get acquainted with FCJ membersRead and take note of internship unit rules and regulations 08/09/2025 08/09/2025 3 Learn about AWS and its types of services:ComputeStorageNetworkingDatabase\u0026hellip; 09/09/2025 09/09/2025 CloudJourney 4 Create AWS Free Tier accountLearn about AWS Console \u0026amp; AWS CLIPractice:Create AWS accountInstall \u0026amp; configure AWS CLIHow to use AWS CLI 10/09/2025 10/09/2025 AWS Account Setup 5 Learn how to write markdown and use Hugo 11/09/2025 11/09/2025 YouTube Guide 6 Practice:Do labs on the First Cloud Journey 12/09/2025 12/09/2025 First Cloud Journey Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region successfully installed hugo and git and VScode:\nknow how to use hugo and git and VScode know how to write markdown and write a successful worklog week 1.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "On this page, you will need to introduce your worklog. How did you complete it? How many weeks did you take to complete the program? What did you do in those weeks?\nTypically, and as a standard, a worklog is carried out over about 3 months (throughout the internship period) with weekly contents as follows:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/",
	"title": "Create a gateway endpoint",
	"tags": [],
	"description": "",
	"content": " Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.1-workshop-overview/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "VPC endpoints VPC endpoints are virtual devices. They are horizontally scaled, redundant, and highly available VPC components. They allow communication between your compute resources and AWS services without imposing availability risks. Compute resources running in VPC can access Amazon S3 using a Gateway endpoint. PrivateLink interface endpoints can be used by compute resources running in VPC or on-premises. Workshop overview In this workshop, you will use two VPCs.\n\u0026ldquo;VPC Cloud\u0026rdquo; is for cloud resources such as a Gateway endpoint and an EC2 instance to test with. \u0026ldquo;VPC On-Prem\u0026rdquo; simulates an on-premises environment such as a factory or corporate datacenter. An EC2 instance running strongSwan VPN software has been deployed in \u0026ldquo;VPC On-prem\u0026rdquo; and automatically configured to establish a Site-to-Site VPN tunnel with AWS Transit Gateway. This VPN simulates connectivity from an on-premises location to the AWS cloud. To minimize costs, only one VPN instance is provisioned to support this workshop. When planning VPN connectivity for your production workloads, AWS recommends using multiple VPN devices for high availability. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.4-s3-onprem/5.4.1-prepare/",
	"title": "Prepare the environment",
	"tags": [],
	"description": "",
	"content": "To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Workshop Report: \u0026ldquo;DevOps on AWS\u0026rdquo; Event Objectives Get a clear understanding of DevOps culture, principles, and how it changes the way teams work Learn how to build CI/CD pipelines using AWS DevOps services Practice Infrastructure as Code with AWS CloudFormation and the AWS CDK Explore container platforms on AWS: ECR, ECS, EKS, and App Runner Set up monitoring and observability with Amazon CloudWatch and AWS X-Ray See how DevOps practices are applied in real production environments Event Details Location: AWS Vietnam Office Date \u0026amp; Time: 8:30 AM – 5:00 PM, Monday, November 17, 2025 Speakers \u0026amp; Facilitators Instructors:\nTruong Quang Tinh – AWS Community Builder DevOps culture and CI/CD fundamentals Van Hoang Kha – AWS Community Builder Infrastructure as Code with CloudFormation Nguyen Khanh Phuc Thinh – AWS Community Builder AWS CDK deep dive Le Huynh Nghiem – AWS Community Builder Container services on AWS Huynh Hoang Long – AWS Community Builder Monitoring and observability Pham Hoang Quy – AWS Community Builder DevOps best practices and real-world stories Facilitators:\nAWS Vietnam Team AWS Community Builders Vietnam Event Agenda 8:30 AM – 9:00 AM: Registration and Opening Check-in and informal networking Welcome talk and overview of the workshop flow Short introduction to key AWS DevOps services 9:00 AM – 10:30 AM: DevOps Culture and CI/CD Pipeline Speaker: Truong Quang Tinh\nDevOps Mindset – shifting from “Dev vs Ops” to “one team”:\nCollaboration: Dev and Ops work together instead of separately Automation: Let tools handle repetitive work so teams can focus on value Continuous Improvement: Use feedback to adjust and improve over time Shared Ownership: Quality, security, and reliability are everyone’s job DORA Metrics – how to know if DevOps is working:\nDeployment Frequency – how often you push changes live Lead Time for Changes – time from commit to production Mean Time to Recovery (MTTR) – how fast you recover from incidents Change Failure Rate – how many releases cause problems Core CI/CD Services on AWS:\nAWS CodeCommit – Git repositories hosted on AWS AWS CodeBuild – build and test automation service AWS CodeDeploy – roll out changes to EC2, Lambda, ECS, or on-prem servers AWS CodePipeline – main pipeline that ties all stages together Deployment Strategies:\nBlue/Green – switch traffic between two environments to reduce downtime Canary – release changes to a small portion of users first Rolling – update instances gradually so the service stays available Live Demo:\nSet up a basic CI/CD pipeline end-to-end using AWS DevOps tools 10:30 AM – 10:45 AM: Coffee Break Short break for coffee and networking 10:45 AM – 12:00 PM: Infrastructure as Code with CloudFormation Speaker: Van Hoang Kha\nCore Ideas of Infrastructure as Code (IaC):\nVersion Control – treat infrastructure like code stored in Git Repeatability – spin up the same environment again and again Living Documentation – templates describe what exists in the cloud Pre-deployment Testing – validate before roll-out CloudFormation Basics:\nTemplates – YAML/JSON files that describe AWS resources Stacks – a group of resources created and managed together Change Sets – preview what will change before you apply it Drift Detection – detect manual changes that differ from the template Best Practices with CloudFormation:\nModular Design – use nested stacks to reuse patterns Parameters – keep templates flexible with input values Outputs – expose useful values for other stacks or teams Cross-Stack References – connect multiple stacks cleanly Advanced Concepts:\nStack Policies – prevent accidental changes to critical pieces Rollback Triggers – roll back when CloudWatch alarms are triggered StackSets – roll out stacks to multiple accounts/regions at once Live Demo:\nDeploy a simple multi-tier app using CloudFormation templates 12:00 PM – 1:00 PM: Lunch Break Lunch and informal discussion with speakers and participants 1:00 PM – 2:15 PM: AWS CDK Deep Dive Speaker: Nguyen Khanh Phuc Thinh\nWhat is AWS CDK?\nInfrastructure described with real programming languages Supports TypeScript, Python, Java, C#, Go, and more Different abstraction layers: L1 – direct CloudFormation mapping L2 – higher-level constructs L3 – ready-made patterns CDK Building Blocks:\nConstructs – reusable building units for cloud apps Stacks – deployment units made of constructs Apps – a collection of stacks deployable as one project Synthesis – CDK code → CloudFormation template CDK vs CloudFormation:\nImperative vs Declarative – use loops, conditions, functions in CDK Type Safety – catch mistakes early via the compiler IDE Support – autocompletion and inline docs help productivity Testing – write tests for your infrastructure code CDK Good Practices:\nUse and share construct libraries Design stacks that are environment-agnostic when possible Keep logical IDs stable to avoid unnecessary replacements Use context values for per-environment configuration Live Demo:\nBuild and deploy a small serverless app using AWS CDK 2:15 PM – 2:30 PM: Coffee Break Short break and more networking 2:30 PM – 3:45 PM: Container Services on AWS Speaker: Le Huynh Nghiem\nContainer Basics:\nWhat containers are and why they’re popular Benefits: consistent environments, portability, resource efficiency Quick review of Docker concepts: images, containers, registries Amazon Elastic Container Registry (ECR):\nPrivate container image registry managed by AWS Built-in image scanning for vulnerabilities Lifecycle policies to auto-delete old images Cross-region replication to speed up deployments globally Amazon Elastic Container Service (ECS):\nAWS-native container orchestration service Task Definitions – recipe for running containers Services – define desired count and keep tasks running Fargate – run containers without managing servers EC2 Launch Type – full control of the underlying instances Amazon Elastic Kubernetes Service (EKS):\nManaged control plane for Kubernetes clusters Use standard Kubernetes APIs and tools Flexible enough for complex microservice architectures Integration with AWS components like ALB, EBS, EFS AWS App Runner:\nSimplest way to go from source code or container image to a running service Handles scaling automatically Integrates with CI/CD flows Ideal for web apps and APIs that don’t need complex orchestration Choosing the Right Container Service:\nECS – good default choice for AWS-focused teams EKS – for teams already familiar with Kubernetes App Runner – when speed and simplicity are top priority Live Demo:\nDeploy a container-based application to ECS and EKS 3:45 PM – 4:45 PM: Monitoring and Observability Speaker: Huynh Hoang Long\nMonitoring vs Observability:\nMonitoring – collecting metrics and checking if things are OK Observability – being able to understand internal behavior from external outputs Focus on the three pillars: logs, metrics, traces Amazon CloudWatch:\nMetrics – track CPU, memory (custom), and app metrics Logs – central place to store and search logs Alarms – trigger alerts or actions based on thresholds Dashboards – custom views for system health Logs Insights – query logs with a SQL-like syntax Events – react to changes in your AWS environment AWS X-Ray:\nDistributed tracing – follow a request across multiple services Service map – visual view of how services talk to each other Trace analysis – identify slow parts and bottlenecks Error analysis – pinpoint where failures happen Works with Lambda, ECS, EKS, API Gateway, and more Good Practices for Observability:\nUse structured logging (JSON or consistent formats) Add custom metrics for business-level KPIs Configure alerts before users notice issues Correlate logs, metrics, and traces for faster debugging Apply retention policies to balance visibility and cost Live Demo:\nSet up dashboards, alarms and tracing for a small microservices system 4:45 PM – 5:00 PM: DevOps Best Practices \u0026amp; Q\u0026amp;A Speaker: Pham Hoang Quy\nDevOps Best Practices:\nAutomate as much as possible: tests, builds, deployments, checks Fail fast and learn: treat incidents as learning opportunities Blameless postmortems: focus on process, not on blaming people Progressive delivery: feature flags, canary, blue/green releases Continuous learning: DevOps is an ongoing improvement cycle Case Studies:\nStartup – use serverless and CI/CD to deliver features quickly Enterprise – multi-account setups with CloudFormation StackSets E-commerce – high-availability deployments using blue/green Q\u0026amp;A Session:\nOpen discussion with participants about tools, culture, and real problems Key Takeaways DevOps Culture and Mindset DevOps is about people, collaboration, and shared goals, not just tools Automation is key to reducing manual work and mistakes DORA metrics give a concrete way to see how the team is improving Continuous improvement is at the heart of DevOps Quality and reliability are shared responsibilities across the team CI/CD Pipeline Automation CodeCommit, CodeBuild, CodeDeploy, and CodePipeline work well together End-to-end automation improves speed and reduces human error Multiple deployment patterns (blue/green, canary, rolling) support safer releases CI/CD can integrate with both AWS-native and third-party services A good pipeline becomes the backbone of the delivery process Infrastructure as Code CloudFormation templates provide a clear and repeatable infrastructure definition CDK adds a more developer-friendly way to define the same resources IaC improves consistency, traceability, and collaboration Drift detection helps catch manual edits that break the IaC model Choosing between CloudFormation and CDK depends on team skills and preference Container Services ECR secures and manages container images with scanning and policies ECS is a strong default for many AWS-based container workloads EKS is powerful for teams already invested in Kubernetes App Runner is ideal when you want “code → service” with minimal steps The right container platform depends on complexity, control, and team experience Monitoring and Observability CloudWatch and X-Ray provide a complete toolkit for visibility on AWS Observability helps understand why something went wrong, not just that it did Proactive alerts are essential for minimizing downtime Logs, metrics, and traces together give a full picture of system health Data from monitoring can guide performance tuning and cost optimizations DevOps Best Practices Automation + culture = effective DevOps, tools alone are not enough Learning from incidents is more important than avoiding them at all costs Progressive delivery reduces risk for every release Regularly reviewing processes and metrics keeps the team moving forward Applying to Work Start with a simple CI/CD pipeline using CodePipeline and expand from there Migrate manual setups into CloudFormation or CDK for consistency Gradually containerize applications and choose ECS/EKS/App Runner where appropriate Strengthen monitoring and observability with CloudWatch dashboards and X-Ray Encourage a DevOps culture of transparency, ownership, and continuous learning Use DORA metrics to track improvement instead of gut feeling Consider aiming for the AWS DevOps Engineer certification as a longer-term goal Event Experience The “DevOps on AWS Workshop” was a full-day, very packed but valuable experience. It covered not only the technical tools, but also the mindset and real-world practices behind DevOps.\nLearning from AWS Experts The speakers provided both high-level concepts and concrete examples Live demos made the theory easier to follow and remember Real case studies helped connect the tools to realistic use cases There was plenty of time to ask specific questions about real problems Hands-on Demonstrations Saw a complete CI/CD pipeline being assembled step by step Watched how IaC with CloudFormation and CDK works in practice Learned how to deploy containers on ECS and EKS Understood what proper monitoring and tracing look like in a microservices setup Understanding DevOps in Practice DevOps was presented as a combination of culture + process + tools DORA metrics provided a clear way to talk about performance and improvement Observability was highlighted as a core requirement for modern systems Examples of incident handling and postmortems were particularly useful Networking and Community Met many engineers who are on the same DevOps journey Shared common struggles like legacy systems, manual processes, and resistance to change Got connected to the AWS DevOps community for future events and learning Personal Takeaways Good DevOps requires both mindset change and technical skills Automation frees teams from “busy work” and lets them focus on real problems Having IaC and proper monitoring in place makes scaling much easier AWS provides almost everything needed to implement DevOps end-to-end Next Steps Build or improve an internal CI/CD pipeline using the ideas from the workshop Refactor existing infrastructure into CloudFormation/CDK where possible Plan a gradual move towards containers for suitable workloads Invest time in setting up meaningful dashboards and alerts Keep learning from AWS documentation, workshops, and the community "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/3-blogstranslated/3.2-blog2/",
	"title": "Updated Carbon Methodology for the AWS Customer Carbon Footprint Tool (CCFT)",
	"tags": [],
	"description": "",
	"content": "(Based on the official AWS update from April 23, 2025)\nTo support customers on their sustainability journey, AWS launched the Customer Carbon Footprint Tool (CCFT) in 2022. CCFT helps customers track and evaluate carbon emissions resulting from their AWS usage. It includes Scope 1 and Scope 2 emissions according to the Greenhouse Gas Protocol, covering all AWS services such as Amazon EC2, Amazon S3, AWS Lambda, and more.\nToday, AWS announces three updates to CCFT:\nEasier access to carbon data through Billing and Cost Management Data Exports. Detailed carbon data broken down by AWS Region. An updated allocation methodology (v2.0), independently assured by APEX. Starting January 2025, CCFT uses methodology v2.0. Data from December 2024 and earlier will continue using v1.0.\n1. Easier Data Access Customers can now export CCFT carbon data via AWS Data Exports.\nKey features:\nProvides emission estimates for all accounts under AWS Organizations. Monthly automated exports delivered to Amazon S3 in CSV or Parquet. First export includes up to 38 months of historical data. Pre-Dec 2024 data uses v1.0; January 2025 onward uses v2.0.\n2. Regional Carbon Granularity Customers can now view carbon emissions broken down by AWS Region.\nCloudFront usage is grouped under Global Services.\nThis enhancement allows customers to:\nIdentify regions contributing the most carbon emissions Make better workload placement decisions 3. Updated Methodology v2.0 Customers often use services across many AWS Regions, making carbon attribution complex.\nMethodology v2.0 is aligned with industry standards including:\nGHG Protocol Corporate Standard GHG Protocol Product Standard ISO 14040/44 (Life Cycle Assessment) ISO 14067 (Product Carbon Footprint) ICT Sector Guidance Scope 1 Overview Scope 1 includes direct emissions from AWS-owned or controlled sources such as backup generators.\nAWS collects annual Scope 1 operational data, calculates emissions at site level, then aggregates them into clusters (AWS Regions or CloudFront edge clusters).\nScope 2 Overview Scope 2 includes indirect emissions from purchased electricity.\nCCFT uses:\nMarket-based methodology Geographic emission factors Grid mix and carbon intensity values verified annually It follows the prioritization rules of the Greenhouse Gas Protocol.\nAllocation Model v2.0 Emission allocation follows three steps:\nAllocate cluster-level emissions to server racks. Allocate rack emissions to AWS services based on resource usage and service dependencies. Allocate service emissions to individual customer accounts. Some customers may see changes in their totals due to improved accuracy.\nThree Key Updates in Methodology v2.0 Unused capacity is now allocated to all AWS customers.\nCarbon associated with unused server capacity is distributed proportionally, as required by GHG Protocol and ISO standards.\nImproved allocation logic for services without dedicated hardware, such as AWS Lambda or Amazon Redshift.\nUpdated allocation of shared overhead, including networking racks and AWS Region expansions.\nMoving Forward AWS will continue improving CCFT as new climate science, data, and customer needs evolve.\nClimate Pledge Commitment AWS remains committed to reaching net-zero carbon by 2040.\nTo learn more, visit the AWS sustainability site.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 2 Objectives: Complete all learning content in Module 01. Understand AWS global infrastructure and service management tools. Practice AWS Budgets for cost optimization. Complete IAM and Billing labs. Learn about AWS Support plans and how to open a support case. Tasks to be carried out this week: Day Task Start End Reference 2 Module 01-04: AWS Global InfrastructureModule 01-05: Service Management Tools 15/09 15/09 FCJ Module 01 3 Module 01-06: Cost OptimizationLab 07-01 → 07-03 16/09 16/09 FCJ Lab 07 4 Lab 07-04 → 07-06Create Cost/Usage/Savings Budget 17/09 17/09 AWS Billing 5 IAM Labs 01-01 → 01-04 18/09 18/09 FCJ Lab 01 6 Learn AWS Support PlansOpen a sample support case 19/09 19/09 AWS Support Docs Week 2 Achievements: Understood how AWS organizes regions, AZs, and infrastructure. Learned to manage cost forecasting with AWS Budgets. Completed all IAM foundational labs. Learned how AWS Support works and how to submit a support request. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Security Scan Pipeline on AWS Cloud Automated solution for source code security analysis in the DevSecOps pipeline 1. BACKGROUND AND PROJECT DRIVERS 1.1 EXECUTIVE SUMMARY The client is developing an application and wants to adopt a DevSecOps model to automate the build–scan–deploy workflow, improve security, and shorten release cycles.\nSystem objectives:\nDetect security vulnerabilities and code smells early in the source code using Amazon CodeGuru Reviewer. Automatically build and create artifacts when new commits occur. Store artifacts securely in Amazon S3. Automatically deploy the application to Amazon EC2 via AWS CodeDeploy. Use cases:\nAutomatic security scanning on new commits. Automated build and artifact creation. Secure artifact storage. Automatic EC2 deployment. Centralized alerts and process monitoring. Professional services provided:\nImplement CodePipeline integrated with GitHub, include CodeBuild + CodeGuru Reviewer, configure S3 artifact storage, deploy to EC2 with CodeDeploy, and enable security monitoring and logging (CloudWatch, CloudTrail, GuardDuty, Security Hub). 1.2 PROJECT SUCCESS CRITERIA The client provides a GitHub repository with appropriate permissions. IAM Roles have sufficient permissions for CodeBuild, CodeDeploy, S3, and CloudWatch. EC2 instances are provisioned and ready for deployment. The application supports deployment via appspec.yml. Multiple environments (dev/stg/prod) are not required at this time. AWS Free Tier is used to reduce costs where possible. Potential risks: timeouts, webhook failures, corrupted artifacts, insufficient IAM permissions. 1.3 ASSUMPTIONS The customer provides a GitHub repository with appropriate permissions. IAM Roles have sufficient permissions for CodeBuild, CodeDeploy, S3, and CloudWatch. EC2 instances are valid and ready for deployment. The application supports deployment via appspec.yml. Multiple environments (dev/stg/prod) are not required at this time. AWS Free Tier is used to minimize costs. Potential risks include: timeouts, webhook failures, corrupted artifacts, insufficient IAM permissions. 2. SOLUTION ARCHITECTURE / ARCHITECTURAL DIAGRAM 2.1 TECHNICAL ARCHITECTURE DIAGRAM Architecture diagram: AWS services used:\nAWS CodePipeline AWS CodeBuild Amazon CodeGuru Reviewer Amazon S3 AWS CodeDeploy Amazon EC2 Amazon SNS CloudWatch, CloudTrail, GuardDuty, Detective, Security Hub 2.2 TECHNICAL PLAN Includes:\nConfigure GitHub connection (OIDC or Personal Access Token). Create CodeBuild project and provide buildspec.yml. Integrate Amazon CodeGuru Reviewer. Configure CodeDeploy and appspec.yml. Enable monitoring and security logging following AWS Well-Architected best practices. 2.3 PROJECT PLAN We will follow Agile Scrum with two sprints (2 weeks per sprint).\nTeam responsibilities:\nDevOps Engineer: design and implement the pipeline Developer: support testing and provide the repository Security Analyst: review alerts from CodeGuru \u0026amp; GuardDuty Meeting cadence:\nDaily Standup Weekly Review Bi-weekly Retrospective Knowledge transfer: one training session on using the pipeline and one session on viewing logs \u0026amp; reports.\n2.4 SECURITY CONSIDERATIONS Access Enable MFA Use IAM Roles instead of long-term Access Keys Infrastructure Security Groups with IP restrictions Data S3 encryption (SSE-S3) Detection GuardDuty, Detective, CloudTrail Incident Management SNS alerts for failed builds/deployments 3. ACTIVITIES \u0026amp; DELIVERABLES 3.1 ACTIVITIES \u0026amp; DELIVERABLES Phase Duration Activity Deliverable Man-day Assessment Week 1 Requirements gathering Initial architecture X Infrastructure setup Week 1–2 S3, IAM, EC2, GitHub Infrastructure X Pipeline setup Week 2–3 CodePipeline + Build + Scan Complete pipeline X Deployment setup Week 3 CodeDeploy Automated deploy X Testing \u0026amp; Go-live Week 4 Test the pipeline Test report X Handover Week 4 Training + documentation Final handover X 3.2 OUT OF SCOPE No additional staging/production environments will be provisioned. No integration with SonarQube / Trivy / Checkov. No advanced dashboard design. No autoscaling or load balancer configuration. 3.3 PATH TO PRODUCTION The POC is intended for the initial demo only. To move to production, additional work is required: automated unit tests, enhanced monitoring, detailed error handling, and multi-environment CI/CD.\n4. AWS COST ESTIMATE Estimated monthly costs:\nCodePipeline: $0.40 / month CodeBuild: $0.35 / month S3: $0.10 / month CodeDeploy: $0.20 / month EC2 t2.micro: $0.10 / month CloudWatch + SNS: $0.05 / month Total: ~ $1.2 / month (~ $14.4 / year)\n5. PROJECT TEAM Executive Sponsor \u0026amp; Stakeholders\nTeam: First Cloud Journey\nProject Team\nName Student ID Email / Contact Lê Công Cảnh SE183750 canhlcse183750@fpt.edu.vn Phùng Gia Đức SE183187 ducpgse183187@fpt.edu.vn Vũ Nguyễn Bình SE193185 vunguyenbinh25@gmail.com Lê Minh Dương SE184079 duonglmse184079@fpt.edu.vn Nguyễn Phi Duy SE180529 duynpse180529@fpt.edu.vn 6. RESOURCES \u0026amp; COST ESTIMATES See the cost estimate on the AWS Pricing Calculator or download the budget estimation file.\n6.1 Resource Allocation \u0026amp; Hourly Rates Resource Responsibility Rate (USD) / Hour Headcount Solution Architects Architecture design, security review, AWS service integration, oversight of CI/CD \u0026amp; scanning pipeline 6 – 9 1 Engineers (DevOps / Cloud) Implement CI/CD pipeline, configure CodePipeline/CodeBuild/CodeDeploy, IAM setup, testing, documentation 4 – 7 1–2 Other (Security Engineer) Integrate code scanning tools (SonarQube, Trivy, CodeGuru Security), analyze reports 5 – 8 1 6.2 Estimated Project Hours by Phase Project Phase Solution Architects (Hours) Engineers (Hours) Other (Hours) Total Hours Phase 1 – Discovery \u0026amp; Requirements 4 4 0 8 Phase 2 – Architecture Design 6 2 0 8 Phase 3 – Pipeline Implementation 4 20 6 30 Phase 4 – Security Scanning Integration 2 6 8 16 Phase 5 – Testing \u0026amp; Validation 2 8 4 14 Phase 6 – Documentation \u0026amp; Handover 2 6 2 10 Total Hours 20 46 20 86 Hours 7. ACCEPTANCE The client has 8 days to review the deliverables. If no response is received, the deliverables will be considered accepted. If defects are found, the provider will fix and resubmit according to the Rejection Notice process. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/",
	"title": "Create an S3 Interface endpoint",
	"tags": [],
	"description": "",
	"content": "In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.2-prerequiste/",
	"title": "Prerequiste",
	"tags": [],
	"description": "",
	"content": "IAM permissions Add the following IAM permission policy to your user account to deploy and cleanup this workshop.\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;VisualEditor0\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;cloudformation:*\u0026#34;, \u0026#34;cloudwatch:*\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:AcceptTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:AllocateAddress\u0026#34;, \u0026#34;ec2:AssociateAddress\u0026#34;, \u0026#34;ec2:AssociateIamInstanceProfile\u0026#34;, \u0026#34;ec2:AssociateRouteTable\u0026#34;, \u0026#34;ec2:AssociateSubnetCidrBlock\u0026#34;, \u0026#34;ec2:AssociateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:AssociateVpcCidrBlock\u0026#34;, \u0026#34;ec2:AttachInternetGateway\u0026#34;, \u0026#34;ec2:AttachNetworkInterface\u0026#34;, \u0026#34;ec2:AttachVolume\u0026#34;, \u0026#34;ec2:AttachVpnGateway\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:AuthorizeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:CreateClientVpnEndpoint\u0026#34;, \u0026#34;ec2:CreateClientVpnRoute\u0026#34;, \u0026#34;ec2:CreateCustomerGateway\u0026#34;, \u0026#34;ec2:CreateDhcpOptions\u0026#34;, \u0026#34;ec2:CreateFlowLogs\u0026#34;, \u0026#34;ec2:CreateInternetGateway\u0026#34;, \u0026#34;ec2:CreateLaunchTemplate\u0026#34;, \u0026#34;ec2:CreateNetworkAcl\u0026#34;, \u0026#34;ec2:CreateNetworkInterface\u0026#34;, \u0026#34;ec2:CreateNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:CreateRoute\u0026#34;, \u0026#34;ec2:CreateRouteTable\u0026#34;, \u0026#34;ec2:CreateSecurityGroup\u0026#34;, \u0026#34;ec2:CreateSubnet\u0026#34;, \u0026#34;ec2:CreateSubnetCidrReservation\u0026#34;, \u0026#34;ec2:CreateTags\u0026#34;, \u0026#34;ec2:CreateTransitGateway\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:CreateTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRoute\u0026#34;, \u0026#34;ec2:CreateTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:CreateTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:CreateVpc\u0026#34;, \u0026#34;ec2:CreateVpcEndpoint\u0026#34;, \u0026#34;ec2:CreateVpcEndpointConnectionNotification\u0026#34;, \u0026#34;ec2:CreateVpcEndpointServiceConfiguration\u0026#34;, \u0026#34;ec2:CreateVpnConnection\u0026#34;, \u0026#34;ec2:CreateVpnConnectionRoute\u0026#34;, \u0026#34;ec2:CreateVpnGateway\u0026#34;, \u0026#34;ec2:DeleteCustomerGateway\u0026#34;, \u0026#34;ec2:DeleteFlowLogs\u0026#34;, \u0026#34;ec2:DeleteInternetGateway\u0026#34;, \u0026#34;ec2:DeleteNetworkInterface\u0026#34;, \u0026#34;ec2:DeleteNetworkInterfacePermission\u0026#34;, \u0026#34;ec2:DeleteRoute\u0026#34;, \u0026#34;ec2:DeleteRouteTable\u0026#34;, \u0026#34;ec2:DeleteSecurityGroup\u0026#34;, \u0026#34;ec2:DeleteSubnet\u0026#34;, \u0026#34;ec2:DeleteSubnetCidrReservation\u0026#34;, \u0026#34;ec2:DeleteTags\u0026#34;, \u0026#34;ec2:DeleteTransitGateway\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPeeringAttachment\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayPrefixListReference\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRoute\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayRouteTable\u0026#34;, \u0026#34;ec2:DeleteTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:DeleteVpc\u0026#34;, \u0026#34;ec2:DeleteVpcEndpoints\u0026#34;, \u0026#34;ec2:DeleteVpcEndpointServiceConfigurations\u0026#34;, \u0026#34;ec2:DeleteVpnConnection\u0026#34;, \u0026#34;ec2:DeleteVpnConnectionRoute\u0026#34;, \u0026#34;ec2:Describe*\u0026#34;, \u0026#34;ec2:DetachInternetGateway\u0026#34;, \u0026#34;ec2:DisassociateAddress\u0026#34;, \u0026#34;ec2:DisassociateRouteTable\u0026#34;, \u0026#34;ec2:GetLaunchTemplateData\u0026#34;, \u0026#34;ec2:GetTransitGatewayAttachmentPropagations\u0026#34;, \u0026#34;ec2:ModifyInstanceAttribute\u0026#34;, \u0026#34;ec2:ModifySecurityGroupRules\u0026#34;, \u0026#34;ec2:ModifyTransitGatewayVpcAttachment\u0026#34;, \u0026#34;ec2:ModifyVpcAttribute\u0026#34;, \u0026#34;ec2:ModifyVpcEndpoint\u0026#34;, \u0026#34;ec2:ReleaseAddress\u0026#34;, \u0026#34;ec2:ReplaceRoute\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupEgress\u0026#34;, \u0026#34;ec2:RevokeSecurityGroupIngress\u0026#34;, \u0026#34;ec2:RunInstances\u0026#34;, \u0026#34;ec2:StartInstances\u0026#34;, \u0026#34;ec2:StopInstances\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsEgress\u0026#34;, \u0026#34;ec2:UpdateSecurityGroupRuleDescriptionsIngress\u0026#34;, \u0026#34;iam:AddRoleToInstanceProfile\u0026#34;, \u0026#34;iam:AttachRolePolicy\u0026#34;, \u0026#34;iam:CreateInstanceProfile\u0026#34;, \u0026#34;iam:CreatePolicy\u0026#34;, \u0026#34;iam:CreateRole\u0026#34;, \u0026#34;iam:DeleteInstanceProfile\u0026#34;, \u0026#34;iam:DeletePolicy\u0026#34;, \u0026#34;iam:DeleteRole\u0026#34;, \u0026#34;iam:DeleteRolePolicy\u0026#34;, \u0026#34;iam:DetachRolePolicy\u0026#34;, \u0026#34;iam:GetInstanceProfile\u0026#34;, \u0026#34;iam:GetPolicy\u0026#34;, \u0026#34;iam:GetRole\u0026#34;, \u0026#34;iam:GetRolePolicy\u0026#34;, \u0026#34;iam:ListPolicyVersions\u0026#34;, \u0026#34;iam:ListRoles\u0026#34;, \u0026#34;iam:PassRole\u0026#34;, \u0026#34;iam:PutRolePolicy\u0026#34;, \u0026#34;iam:RemoveRoleFromInstanceProfile\u0026#34;, \u0026#34;lambda:CreateFunction\u0026#34;, \u0026#34;lambda:DeleteFunction\u0026#34;, \u0026#34;lambda:DeleteLayerVersion\u0026#34;, \u0026#34;lambda:GetFunction\u0026#34;, \u0026#34;lambda:GetLayerVersion\u0026#34;, \u0026#34;lambda:InvokeFunction\u0026#34;, \u0026#34;lambda:PublishLayerVersion\u0026#34;, \u0026#34;logs:CreateLogGroup\u0026#34;, \u0026#34;logs:DeleteLogGroup\u0026#34;, \u0026#34;logs:DescribeLogGroups\u0026#34;, \u0026#34;logs:PutRetentionPolicy\u0026#34;, \u0026#34;route53:ChangeTagsForResource\u0026#34;, \u0026#34;route53:CreateHealthCheck\u0026#34;, \u0026#34;route53:CreateHostedZone\u0026#34;, \u0026#34;route53:CreateTrafficPolicy\u0026#34;, \u0026#34;route53:DeleteHostedZone\u0026#34;, \u0026#34;route53:DisassociateVPCFromHostedZone\u0026#34;, \u0026#34;route53:GetHostedZone\u0026#34;, \u0026#34;route53:ListHostedZones\u0026#34;, \u0026#34;route53domains:ListDomains\u0026#34;, \u0026#34;route53domains:ListOperations\u0026#34;, \u0026#34;route53domains:ListTagsForDomain\u0026#34;, \u0026#34;route53resolver:AssociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:AssociateResolverRule\u0026#34;, \u0026#34;route53resolver:CreateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:CreateResolverRule\u0026#34;, \u0026#34;route53resolver:DeleteResolverEndpoint\u0026#34;, \u0026#34;route53resolver:DeleteResolverRule\u0026#34;, \u0026#34;route53resolver:DisassociateResolverEndpointIpAddress\u0026#34;, \u0026#34;route53resolver:DisassociateResolverRule\u0026#34;, \u0026#34;route53resolver:GetResolverEndpoint\u0026#34;, \u0026#34;route53resolver:GetResolverRule\u0026#34;, \u0026#34;route53resolver:ListResolverEndpointIpAddresses\u0026#34;, \u0026#34;route53resolver:ListResolverEndpoints\u0026#34;, \u0026#34;route53resolver:ListResolverRuleAssociations\u0026#34;, \u0026#34;route53resolver:ListResolverRules\u0026#34;, \u0026#34;route53resolver:ListTagsForResource\u0026#34;, \u0026#34;route53resolver:UpdateResolverEndpoint\u0026#34;, \u0026#34;route53resolver:UpdateResolverRule\u0026#34;, \u0026#34;s3:AbortMultipartUpload\u0026#34;, \u0026#34;s3:CreateBucket\u0026#34;, \u0026#34;s3:DeleteBucket\u0026#34;, \u0026#34;s3:DeleteObject\u0026#34;, \u0026#34;s3:GetAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:GetBucketAcl\u0026#34;, \u0026#34;s3:GetBucketOwnershipControls\u0026#34;, \u0026#34;s3:GetBucketPolicy\u0026#34;, \u0026#34;s3:GetBucketPolicyStatus\u0026#34;, \u0026#34;s3:GetBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:GetObjectVersion\u0026#34;, \u0026#34;s3:GetBucketVersioning\u0026#34;, \u0026#34;s3:ListAccessPoints\u0026#34;, \u0026#34;s3:ListAccessPointsForObjectLambda\u0026#34;, \u0026#34;s3:ListAllMyBuckets\u0026#34;, \u0026#34;s3:ListBucket\u0026#34;, \u0026#34;s3:ListBucketMultipartUploads\u0026#34;, \u0026#34;s3:ListBucketVersions\u0026#34;, \u0026#34;s3:ListJobs\u0026#34;, \u0026#34;s3:ListMultipartUploadParts\u0026#34;, \u0026#34;s3:ListMultiRegionAccessPoints\u0026#34;, \u0026#34;s3:ListStorageLensConfigurations\u0026#34;, \u0026#34;s3:PutAccountPublicAccessBlock\u0026#34;, \u0026#34;s3:PutBucketAcl\u0026#34;, \u0026#34;s3:PutBucketPolicy\u0026#34;, \u0026#34;s3:PutBucketPublicAccessBlock\u0026#34;, \u0026#34;s3:PutObject\u0026#34;, \u0026#34;secretsmanager:CreateSecret\u0026#34;, \u0026#34;secretsmanager:DeleteSecret\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:ListSecrets\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34;, \u0026#34;secretsmanager:PutResourcePolicy\u0026#34;, \u0026#34;secretsmanager:TagResource\u0026#34;, \u0026#34;secretsmanager:UpdateSecret\u0026#34;, \u0026#34;sns:ListTopics\u0026#34;, \u0026#34;ssm:DescribeInstanceProperties\u0026#34;, \u0026#34;ssm:DescribeSessions\u0026#34;, \u0026#34;ssm:GetConnectionStatus\u0026#34;, \u0026#34;ssm:GetParameters\u0026#34;, \u0026#34;ssm:ListAssociations\u0026#34;, \u0026#34;ssm:ResumeSession\u0026#34;, \u0026#34;ssm:StartSession\u0026#34;, \u0026#34;ssm:TerminateSession\u0026#34; ], \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Provision resources using CloudFormation In this lab, we will use N.Virginia region (us-east-1).\nTo prepare the workshop environment, deploy this CloudFormation Template (click link): PrivateLinkWorkshop . Accept all of the defaults when deploying the template.\nTick 2 acknowledgement boxes Choose Create stack The ClouddFormation deployment requires about 15 minutes to complete.\n2 VPCs have been created 3 EC2s have been created "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/",
	"title": "Test the Gateway Endpoint",
	"tags": [],
	"description": "",
	"content": "Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/4-eventparticipated/4.3-event3/",
	"title": "Event 3",
	"tags": [],
	"description": "",
	"content": "Summary Report: \u0026ldquo;AWS Well-Architected Security Pillar Workshop\u0026rdquo; Event Details Date: November 29, 2025 — Morning Only Time: 08:30 AM – 12:00 PM Location: AWS Vietnam Office Speakers Le Vu Xuan An – AWS Cloud Club Captain HCMUTE Tran Duc Anh – AWS Cloud Club Captain SGU Tran Doan Cong Ly – AWS Cloud Club Captain PTIT Danh Hoang Hieu Nghi – AWS Cloud Club Captain HUFLIT 8:30 – 8:50 AM | Opening \u0026amp; Security Foundation Role of the Security Pillar within the Well-Architected Framework Core principles: Least Privilege, Zero Trust, Defense in Depth Shared Responsibility Model Common cloud threats in Vietnam Pillar 1 — Identity \u0026amp; Access Management 8:50 – 9:30 AM | Modern IAM Architecture IAM basics: Users, Roles, Policies – avoiding long-term credentials IAM Identity Center: SSO and permission sets SCPs and permission boundaries for multi-account setups MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access Pillar 2 — Detection 9:30 – 9:55 AM | Detection \u0026amp; Continuous Monitoring Organization-level CloudTrail, GuardDuty, Security Hub Logging across all layers: VPC Flow Logs, ALB Logs, S3 Access Logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules as code) 9:55 – 10:10 AM | Coffee Break Pillar 3 — Infrastructure Protection 10:10 – 10:40 AM | Network \u0026amp; Workload Security VPC segmentation: private vs public placement Security Groups vs NACLs and when to use each WAF, Shield, Network Firewall Workload protection fundamentals: EC2, ECS/EKS Pillar 4 — Data Protection 10:40 – 11:10 AM | Encryption, Keys \u0026amp; Secrets KMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit for S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store best practices Data classification and access guardrails Pillar 5 — Incident Response 11:10 – 11:40 AM | IR Playbook \u0026amp; Automation AWS Incident Response lifecycle Example playbooks: Compromised IAM key Public S3 exposure EC2 malware detection Snapshotting, isolation \u0026amp; evidence collection Automated response with Lambda / Step Functions 11:40 AM – 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of all 5 pillars Common pitfalls \u0026amp; real examples from Vietnamese companies Recommended learning path (Security Specialty, SA Pro) "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/3-blogstranslated/3.3-blog3/",
	"title": "Using AWS Service Reference Information to Automate Policy Management Workflows",
	"tags": [],
	"description": "",
	"content": "AWS provides a comprehensive service reference dataset that helps organizations manage their AWS service usage more securely and efficiently. This dataset includes detailed information about IAM permissions, data, APIs, supported actions, and conditions for every AWS service. Customers can use this dataset to automate the creation, review, and management of IAM policies.\nBelow is an overview of how AWS describes the use of this information to automate policy management workflows.\n1. Purpose of the AWS Service Reference Dataset The dataset helps organizations:\nAutomate IAM policy creation. Analyze usage to identify excessive permissions. Reduce over-privileged access following the least-privilege principle. Integrate into review, audit, or change-management workflows. The dataset includes:\nAll supported actions for each AWS service. IAM permissions required for those actions. The resource types each action applies to. Supported condition keys. 2. Core Usage Models Organizations can apply the AWS service reference dataset across multiple use cases:\nAutomated IAM Policy Generation Using CloudTrail data together with the dataset, organizations can:\nIdentify the actions an application actually performs. Generate just-in-time IAM policies based on real usage. Avoid granting unnecessary permissions. Policy Optimization Automated tooling can:\nDetect unused actions. Suggest removal of excessive permissions. Highlight differences between granted and used permissions. Assisting Accurate Policy Authoring The dataset provides:\nCorrect IAM action names. Supported resource types for each action. Applicable condition keys. This reduces errors when writing policies manually.\nAutomating Policy Review and Approval Organizations can use the dataset to:\nValidate policies before approval. Check whether an IAM action is valid. Automatically reject policies requesting unnecessary permissions. 3. Available Data in the Reference Dataset The dataset includes:\nAWS service list. IAM action sets for each service. Supported resource types for each action. Allowed condition keys. Documentation mappings for permissions and APIs. AWS updates the dataset monthly.\n4. Automating Policy Management Workflows A complete system may combine:\nCloudTrail (real API usage logs). The IAM service reference dataset (to validate actions and resources). A policy review and approval pipeline. Typical workflow:\nCollect API usage from CloudTrail. Compare against IAM reference data to determine required permissions. Generate or suggest policy updates. Submit the policy to an approval pipeline. Automatically apply approved policy changes. 5. Key Benefits Stronger security through least-privilege access. Automated workflows reduce manual effort. Lower risk of human errors. Better visibility into permission changes. Smooth integration with DevOps processes. 6. Conclusion The AWS service reference dataset provides a foundational resource enabling organizations to automate IAM policy management. Combined with CloudTrail and workflow automation, it allows teams to build precise, secure, and intelligent policy systems with reduced risk and improved governance.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 3 Objectives: Understand AWS networking fundamentals: VPC, Subnets, Route Tables. Perform VPC-related labs in Module 02. Learn the differences between Security Groups and NACLs. Tasks to be carried out this week: Day Task Start End Reference 2 Watch Module 02-01 → 02-02Review VPC theory 22/09 22/09 FCJ Module 02 3 Create VPC and SubnetsConfigure Route Tables 23/09 23/09 Lab 03 4 Create Internet GatewayCreate NAT Gateway 24/09 24/09 VPC Labs 5 Configure Security GroupsCompare SG vs NACL 25/09 25/09 Module 02 6 Test connectivity between subnets (ping/SSH) 26/09 26/09 Lab Test Week 3 Achievements: Understood the structure of AWS networking. Created a fully functional VPC setup. Learned when to use SGs and NACLs appropriately. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "This section will list and introduce the blogs you have translated. For example:\nBlog 1 - Using Large Language Models on Amazon Bedrock for Multi-step Task Execution Blog 2 - Updated Carbon Methodology for the AWS Customer Carbon Footprint Tool (CCFT) Blog 3 - Using AWS Service Reference Information to Automate Policy Management Workflows "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.3-s3-vpc/",
	"title": "Access S3 from VPC",
	"tags": [],
	"description": "",
	"content": "Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/",
	"title": "Test the Interface Endpoint",
	"tags": [],
	"description": "",
	"content": "Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 4 Objectives: Learn EC2 concepts: Instances, AMI, EBS, Metadata. Practice launching EC2 and attaching EBS. Understand Autoscaling and Load Balancing. Perform EC2 backup using AWS Backup. Tasks to be carried out this week: Day Task Start End Reference 2 Watch Module 03-01EC2 types, pricing, AMI 29/09 29/09 Module 03 3 Create EC2 + EBSUse User Data, Metadata 30/09 30/09 EC2 Docs 4 Create Load BalancerSet up Autoscaling Group 01/10 01/10 EC2 Labs 5 Perform AWS Backup (Lab13)Create backup plan 02/10 02/10 AWS Backup 6 Restore EC2 from backup 03/10 03/10 Backup Lab Week 4 Achievements: Learned full EC2 workflow. Implemented Load Balancer + Auto Scaling. Successfully backed up and restored an EC2 instance. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/4-eventparticipated/",
	"title": "Events Participated",
	"tags": [],
	"description": "",
	"content": " In this section, you should list and describe in detail the events you have participated in during your internship or work experience.\nEach event should be presented in the format Event 1, Event 2, Event 3…, along with the following details:\nEvent name Date and time Location (if applicable) Your role in the event (attendee, event support, speaker, etc.) A brief description of the event’s content and main activities Outcomes or value gained (lessons learned, new skills, contribution to the team/project) This listing helps demonstrate your actual participation as well as the soft skills and experience you have gained from each event. During my internship, I participated in two events. Each one was a memorable experience that provided new, interesting, and useful knowledge, along with gifts and wonderful moments.\nEvent 1 Event Name: AI/ML/GenAI on AWS\nDate \u0026amp; Time: 8:30 AM – 12:00 PM, Saturday, November 15, 2025\nLocation: 36th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: DevOps on AWS\nDate \u0026amp; Time: 8:30 AM – 5:00 PM, Monday, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Well-Architected Security Pillar Workshop Date \u0026amp; Time: 08:30 AM – 12:00 PM, November 29, 2025 — Morning Only\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.4-s3-onprem/",
	"title": "Access S3 from on-premises",
	"tags": [],
	"description": "",
	"content": "Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/",
	"title": "On-premises DNS Simulation",
	"tags": [],
	"description": "",
	"content": "AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 5 Objectives: Master S3 bucket operations: ACL, versioning, replication. Host a static website using S3. Use CloudFront for CDN distribution. Tasks to be carried out this week: Day Task Start End Reference 2 Module 04-02: S3 basicsCreate S3 bucket 06/10 06/10 S3 Docs 3 Enable Static Website HostingManage Public Access Block 07/10 07/10 S3 Lab 4 Create CloudFront distribution 08/10 08/10 CloudFront Docs 5 Enable VersioningConfigure Cross-region Replication 09/10 09/10 Advanced S3 6 Test website + CloudFront routing 10/10 10/10 Lab Test Week 5 Achievements: Mastered S3 bucket configuration. Built a complete static website hosting solution. Distributed content via CloudFront successfully. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.5-policy/",
	"title": "VPC Endpoint Policies",
	"tags": [],
	"description": "",
	"content": "When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Secure Hybrid Access to S3 using VPC Endpoints Overview AWS PrivateLink provides private connectivity to AWS services from VPCs and your on-premises networks, without exposing your traffic to the Public Internet.\nIn this lab, you will learn how to create, configure, and test VPC endpoints that enable your workloads to reach AWS services without traversing the Public Internet.\nYou will create two types of endpoints to access Amazon S3: a Gateway VPC endpoint, and an Interface VPC endpoint. These two types of VPC endpoints offer different benefits depending on if you are accessing Amazon S3 from the cloud or your on-premises location\nGateway - Create a gateway endpoint to send traffic to Amazon S3 or DynamoDB using private IP addresses.You route traffic from your VPC to the gateway endpoint using route tables. Interface - Create an interface endpoint to send traffic to endpoint services that use a Network Load Balancer to distribute traffic. Traffic destined for the endpoint service is resolved using DNS. Content Workshop overview Prerequiste Access S3 from VPC Access S3 from On-premises VPC Endpoint Policies (Bonus) Clean up "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 6 Objectives: Learn IAM advanced concepts: Role, Policy, Access Control. Practice IAM Role creation for EC2 and Lambda. Explore AWS Security Hub and GuardDuty. Tasks to be carried out this week: Day Task Start End Reference 2 Module 05-01 → 05-03: IAM, Cognito 13/10 13/10 Module 05 3 Create IAM Roles (EC2/Lambda)Inline vs Managed Policy 14/10 14/10 IAM Docs 4 Practice Switch RoleUse IAM Policy Simulator 15/10 15/10 IAM Tools 5 Enable Security HubEnable GuardDuty 16/10 16/10 AWS Security 6 Review all Security fundamentals 17/10 17/10 Notes Week 6 Achievements: Understood IAM deep concepts. Created and tested IAM Roles correctly. Enabled and analyzed GuardDuty \u0026amp; Security Hub findings. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/5-workshop/5.6-cleanup/",
	"title": "Clean up",
	"tags": [],
	"description": "",
	"content": "Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/6-self-evaluation/",
	"title": "Self-Assessment",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services Vietnam Co., Ltd. from September 8th, 2025 to December 12th, 2025, I had the opportunity to apply what I learned in school to real-world tasks and observe how large-scale cloud environments operate.\nBy joining the First Cloud Journey program, I improved my skills in communication, teamwork, time management, and gained deeper knowledge in AWS services and cloud practices.\nThroughout the internship, I consistently aimed to complete my tasks on time, follow company guidelines, and collaborate actively with my teammates and mentors.\nBelow is my self-evaluation:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Ability to apply AWS/Cloud concepts, work quality, tool proficiency ✅ ☐ ☐ 2 Ability to learn Speed of learning, adaptability, willingness to explore new technologies ☐ ✅ ☐ 3 Proactiveness Taking initiative, not waiting passively for instructions ✅ ☐ ☐ 4 Responsibility Completing tasks on time and ensuring accuracy ✅ ☐ ☐ 5 Discipline Following schedules, procedures, and workplace rules ☐ ✅ ☐ 6 Growth mindset Accepting feedback and continuously improving ✅ ☐ ☐ 7 Communication Clear expression of ideas and progress ☐ ✅ ☐ 8 Teamwork Ability to collaborate effectively with the team ✅ ☐ ☐ 9 Professional behavior Respectful attitude and proper workplace conduct ✅ ☐ ☐ 10 Problem-solving skills Ability to analyze issues and propose practical solutions ☐ ✅ ☐ 11 Contribution to project/team Completing assigned tasks, supporting team progress ☐ ✅ ☐ 12 Overall performance General evaluation of the internship ☐ ✅ ☐ Areas for improvement: Strengthen personal discipline and consistency in work habits. Improve analytical thinking and approach to problem-solving. Handle technical issues more quickly and safely. Set clearer goals and plan work more effectively. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives: Learn the basics of database services on AWS. Get familiar with Amazon RDS, Aurora, DynamoDB. Understand backup, restore, and database security. Complete Module 06 labs. Tasks to be carried out this week: Day Task Start End Reference 2 Watch Module 06-01 → 06-02Learn RDS \u0026amp; Aurora basics 20/10 20/10 Module 06 3 Create RDS Subnet GroupCreate DB Security Group 21/10 21/10 RDS Lab 4 Launch RDS instanceTest connection from EC2 22/10 22/10 Lab 05 5 Perform DB backup \u0026amp; restoreReview snapshot usage 23/10 23/10 RDS Docs 6 Study DynamoDB \u0026amp; ElastiCache 24/10 24/10 NoSQL Docs Week 7 Achievements: Understood SQL vs NoSQL differences on AWS. Successfully created and connected to an RDS database. Learned how to configure DB backup, snapshot, and restore. Gained basic understanding of DynamoDB and ElastiCache. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment at FCJ is comfortable and easy to adapt to. Everyone is supportive and willing to help whenever I run into difficulties, even outside learning hours. The workspace is organized and quiet, which helps me stay focused. It would be even better if we had more team bonding sessions to get to know each other more.\n2. Support from Mentor / Team Admin\nMy mentor provides clear explanations and guides me step-by-step while still giving me space to try solving things on my own first. The admin team is very helpful with documents, logistics, and creating a smooth experience throughout the internship. I really appreciate that my mentor encourages me to ask questions instead of being afraid of making mistakes.\n3. Relevance to My Major\nMost of the tasks match the subjects I studied at university, and I was also exposed to many new topics such as cloud operations and DevOps practices. This mixture helped me reinforce my foundation while gaining hands-on experience.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring this internship, I learned how to use new technical tools, improved my teamwork ability, and practiced professional communication. My mentor also shared real-world insights that helped me understand more clearly how the cloud industry works and what skills I need moving forward.\n5. Team Culture \u0026amp; Collaboration\nThe culture in FCJ is very positive. People respect each other and keep a friendly atmosphere even when the workload increases. I always felt included as part of the team, despite being an intern.\n6. Internship Policies \u0026amp; Benefits\nAlthough there is no internship allowance, the flexible scheduling and chances to join internal workshops are very valuable to me.\nAdditional Questions What did you find most satisfying during your internship?\nBeing guided closely by mentors and receiving clear feedback on how to improve. What should the company improve for future interns?\nAt the moment, I don’t have any suggestions. Would you recommend your friends to intern here? Why?\nYes. The FCJ program is extremely helpful and provides solid knowledge for future career development. Suggestions \u0026amp; Expectations Any suggestions to improve the internship experience?\nI hope AWS will allow interns to visit the office more frequently. Would you like to continue this program in the future?\nYes, because AWS is a great place to grow professionally and offers many opportunities in cloud-related career paths. Any other comments?\nNothing additional, except my appreciation to my mentor and the FCJ team for all their support throughout the internship. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives: Kick off the “Security Scan Pipeline on AWS Cloud” project. Review project requirements and define team roles. Analyze the architecture of a DevSecOps pipeline. Prepare the GitHub repository and initial documentation. Tasks to be carried out this week: Day Task Start End Reference 2 Team meeting to assign rolesReview and analyze project proposal 27/10 27/10 Project Docs 3 Study DevSecOps pipeline architectureReview CodePipeline, CodeBuild, CodeDeploy 28/10 28/10 AWS DevOps Docs 4 Create the initial architecture draft 29/10 29/10 Diagram Tools 5 Create GitHub repoAssign permissions to members 30/10 30/10 GitHub 6 Review buildspec.yml \u0026amp; appspec.ymlPrepare needed IAM permissions 31/10 31/10 CI/CD Docs Week 8 Achievements: Fully understood project scope and requirements. Completed the first draft of the pipeline architecture. Set up GitHub repo and prepared required documentation. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives: Finalize the design of the Security Scan Pipeline. Build the foundational AWS infrastructure. Connect GitHub to AWS CodePipeline. Tasks this week: Day Task Start End Reference 2 Finalize pipeline architecture diagram 03/11 03/11 Project Docs 3 Create required IAM RolesCodeBuildRole, CodeDeployRole 04/11 04/11 IAM Docs 4 Create S3 Bucket for artifactsEnable encryption 05/11 05/11 S3 Docs 5 Launch EC2 instance for deploymentConfigure security groups 06/11 06/11 EC2 Docs 6 Connect GitHub → CodePipelineTest Source stage 07/11 07/11 GitHub Docs Week 9 Achievements: Pipeline architecture finalized. Core infrastructure (IAM, S3, EC2) successfully deployed. GitHub integrated with CodePipeline without issues. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Build the CI/CD pipeline structure. Integrate CodeGuru Reviewer for automated security scanning. Create buildspec.yml and appspec.yml. Tasks this week: Day Task Start End Reference 2 Create CodePipeline: Source → Build → Scan → Deploy 10/11 10/11 CodePipeline Docs 3 Configure CodeBuildWrite buildspec.yml 11/11 11/11 CodeBuild Docs 4 Integrate CodeGuru Reviewer 12/11 12/11 CodeGuru Docs 5 Create appspec.yml for deployment 13/11 13/11 CodeDeploy Docs 6 Run pipeline test — fix IAM permission errors 14/11 14/11 Troubleshooting Notes Week 10 Achievements: Pipeline functional through Source, Build, and Scan stages. CodeGuru successfully scanning code for security issues. Deployment configuration ready for Week 11. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Complete the Deploy stage. Perform full pipeline testing. Fix issues and improve workflow stability. Tasks this week: Day Task Start End Reference 2 Configure CodeDeploy Application \u0026amp; Deployment Group 17/11 17/11 CodeDeploy Docs 3 Review AppSpec lifecycle hooks 18/11 18/11 AppSpec Docs 4 First end-to-end pipeline test → EC2 permission error 19/11 19/11 Test Logs 5 Fix IAM Role \u0026amp; adjust appspec.yml 20/11 20/11 Troubleshooting 6 Enable CloudWatch, GuardDuty, Security Hub 21/11 21/11 AWS Security Docs Week 11 Achievements: Successfully deployed application to EC2 via CodeDeploy. Full pipeline now running smoothly end-to-end. Security monitoring tools activated and functioning. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives: Complete all project documentation. Gather logs, test results, and error reports. Prepare the presentation slides. Finalize the internship report. Tasks this week: Day Task Start End Reference 2 Collect pipeline logs from CloudWatch 24/11 24/11 CloudWatch 3 Write pipeline operation guide 25/11 25/11 Project Docs 4 Prepare handover package (architecture, configs, diagrams) 26/11 26/11 Deliverables 5 Document errors and troubleshooting steps 27/11 27/11 Troubleshooting Notes 6 Create final presentation slidesReview all worklogs from Week 1–12 28/11 28/11 Final Review Week 12 Achievements: Completed all technical documentation for the project. Prepared full test logs and error reports. Pipeline is ready for final demonstration. Internship report finalized. "
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://minhDuong27.github.io/fcj-workshop1/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]